![[Pasted image 20250917133105.png]]![[Pasted image 20250917133136.png]]
![[Pasted image 20250917133659.png]]
- มันมีหลายวิธีสำหรับการแก้ไขปัญหา ไม่จำเป็นที่จะต้องไปจบที่ Fine tuning หรือ Train เสมอไป อาจจะแค่ RAG
- เดี๊ยวนี้ LLM มันมีความฉลาดมาก บางทีงานมันอาจจะจบลงที่ การทำ Prompt Engineer ก็ได้
- บางวิธีคือไล่จากถูก -> แพง
- ==RAG== การทำตัวนี้ไม่ได้แพง
- ==Trained== ตัวนี้อาจจะ Take resource (A100) เยอะและจะต้องมีการเตรียมข้อมูล
- ในเคสที่ข้อมูลมีรูปภาพ, ตาราง หรือว่ามีข้อความเยอะการทำ Prompt Engineer อาจจะไม่เหมาะสมแล้วสิ่งที่จะต้องทำก็คือ การทำ RAG , Vector search, semantic search เพื่อให้ LLM ลด Hallucination
### Simple RAG
- ![[Pasted image 20250925131038.png]]
- Simple RAG (Vector Database Workflow) : 
	1. User input : Front-end อาจจะเป็น React หรือไม่ก็ stream lit
	2. Retriever system : เป็นระบบที่เอาไว้ค้นข้อมูลใน database สำหรับการทำ search ต่างๆ
	3. Generative system (LLM) : ใช้ context ในการสร้างคำตอบที่ถูกต้อง
![[Pasted image 20250917133816.png]]
-  External source เป็นหัวใจของ RAG เป็นเหมือนคลังความรู้ขนาดใหญ่
### Enterprise grade for RAG
![[01.png]]
[[How_to_Architect_an_Enterprise_RAG_System]]
- ==Embedding== 
	- เป็นเหมือนกับการแปลงสิ่งที่คอมไม่เข้าใจให้สามารถเข้าใจได้ ไม่ว่าจะเป็นข้อความ หรือ รูปภาพ
	- ทำให้เราสามารถเก็บข้อมูลเอาไว้ใน  (Vector) Database ได้  
- ==อันนี้คือ RAG ที่ดีที่จะสามารถส่งมอบได้มันจะมีอยู่ด้วยกัน 17 ส่วนประกอบ==
	- 01_User authentication 
		- มันคือระบบยืนยันตัวจัดการสิทธิการเข้าถึง
		- ปกป้องการเข้าถึง RAG
		- ตัวอย่าง ==AWS recognito (Cloud)==, ==firebase authentication==
	- 02_Input guardrail
		- เป็นการกรองข้อมูลที่ผิดปกติ ก่อนเข้าระบบ 
		- การห้าม prompt, injection อะไรก็ตามที่มันไม่เหมาะสม
		- ข้อมูลส่วนตัว
		- ตัวอย่าง ==AWS Bedrock guardrail==
	- 03 Query rewriter
		- ระบบที่ช่วยปรับปรุงคำถาม ให้มันมีประสิทธิภาพมากขึ้น 
			- อาจจะเป็นการเอาประวัติก่อนๆ มาสร้างให้คำถามดีขึ้น
			- แกะคำถามยาวๆ ให้ออกมาเป็นคำถามย่อยๆ
		- เราจะไม่ใส่คำถามเข้าไปตรงๆ แต่เราจะเอามันมาเขียนคำถามใหม่ก่อน
	- 04 Encoder
		- ตัวแปลงข้อความเป็น Vector
		- การใช้ Model แปลงข้อความให้เป็น Vector สำหรับทำการค้นหาด้วย Semantic search หรือตัวอื่นๆ
		- วิธีการเลือก Model มันก็มีความสำคัญ เพราะจะได้มีประสิทธิภาพมากที่สุดไม่ว่าจะเป็น
			- การเลือก OpenAI, HuggingFace, Cohere
			- เราจะ Focus กันที่ภาษาอะไร ? ไทย หรือ อังกฤษ หรือทั้งคู่
			- ประเภทข้อมูลเป็นยังไงเป็นแค่ Text หรือ มีอย่างอื่นอย่างเช่นรูปภาพด้วย
	- 05 Document ingestion
		- เป็นระบบป้อนข้อมูล เป็นการแปลงไฟล์เอกสาร -> ตัดเป็น chunk -> ใส่เข้าไว้ใน Database
		- แปลงไฟล์ word, PDF, HTML -> ข้อความ 
		- ตัดแบ่ง Chunk ของเอกสาร Embed เข้า Database
		- การแบ่ง Chunk ที่เหมาะสมจะขึ้นอยู่กับ Context
			- Chunk ละเอียดเกินไปมันจะเกิด Noise เยอะ
			- Chunk เล็กเกินไปเนื้อหาจะกระจายและ Context ไม่ครบ
		- มันก็จะมีตัว Indexer ที่จะเป็นการแบ่งข้อมูลรวมถึงการฝั่ง Metadata ให้อยู่ในรูปแบบที่สามารถทำให้การค้นหาเนี้ยสามารถทำได้รวดเร็วมากขึ้น
		- รองรับการ Update ข้อมูลแบบ Realtime
	- 06 Data Storage
		- เป็นการเก็บข้อมูลหลายประเภทเช่น Embedding, Chat history, Feedback ซึ่งจะเก็บเหล่านี้แยกจากกัน
		- เราจะต้องเลือกประเภทของ Storage ให้เหมาะสมอย่างเช่น
			- MongoDB -> Rawtext
			- PineCone -> Vecter
		- สำหรับระบบที่เป็นระดับ Production ใหญ่ๆแล้วเราจำเป็นจะต้องมี database หลายๆ แบบ
		- เวลาที่เราจะต้องเก็บข้อมูลเราจะต้องพิจารณาก่อนว่าเป็นรูปแบบไหน Database ตัวนั้นควรจะเป็นครอบครุมทั้ง Hybrid search และ Vanilla search
			-   เช่น Weaviate / Qdrant
	- 07 HyDE [[Retrieval-efficiency techniques]]
		- เพิ่มคุณภาพก่อนที่จะเอาไปทำ Encoder
		- จะเป็นส่วนที่ LLM synthetic data ขึ้นมาเพื่อให้เอกสารสมบูรณ์
		- ทำให้สามารถค้นหาที่ดีขึ้น
	- 08 [[Improve ranking]]
		- การเพิ่มคุณภาพของการค้น
	- 09 Generator
		- คือ LLM ที่สร้างคำตอบ
		- อาจจะเป็นแบบ Self-host หรือเป็นการเรียกผ่าน API
		- ถ้าหากว่าเป็น Local อาจจะใช้เป็น Ollama
		- ต้อง Optimize lantency (เพื่อ streamming) ให้ดี อาจจะเป็นการทำ 
			- Batching 
			- Kansel parellel
			- Quantize lora
	- 10 Prompt technique
		- Chain of thought
		- Selfarch เพื่อให้ LLM Grounded มากกว่าเดิม
	- 11 Output Guardrail
		- เป็นตัวกรองผลลัพธ์ขาออก
		- ตรวจสอบก่อนส่งกลับไปให้ user อย่างเช่น Halluciation
		- ข้อมูลผิดกฏหมาย
	- 12 User feedback loop
		- feedback ที่ได้รับกลับมาจาก User ผู้ใช้งาน
		- ระบบ RAG ที่ดีต้องมีคนอยู่ใน System ด้วยเรียนว่า Human feedback loop
	- 13 Observability
		- การทำ Monitoring 
			- ความเร็ว
			- คุณภาพของคำตอบ
			- อัตราการ Hallucinate 
			- จำนวน Token
		- มีการทำ Test และ Log 
		- Caching
			- เป้นการเก็บคำถามและตอบเอาไว้ในกรณีที่ User ถามคำถามซ้ำๆ จะได้ตอบได้โดยที่ไม่ต้องกิน resource และเร้ว
- คำถามของ user จะถูกส่งมาเป็น vector embedding

### RAG system problem
![[02.png]]
[[7_Failure_Points_of_RAG_Systems]] 
- 7 ปัญหาของการทำ RAG
	- 01 Missing content
		- เวลา user ถามคำถามมาแล้วมันไม่มีข้อมูลที่จะสามารถตอบได้
		- สิ่งที่ควรเกิดขึ้นมันควรตอบว่าไม่รู้ แต่มันดัน Hallusinate ทำให้มีคำตอบมั่วๆ มันพยายามที่จะเดาคำตอบขึ้นมาเอง
	- 02 Missed Top Ranked
		- คำตอบมีอยู่ในระบบแต่ว่าเอกสารไม่ได้โดนดึงออกมา
		- ปกติแล้วการตอบตำถามของ RAG เนี้ยมันจะเป็นในรูปแบบของ Top K
		- เราอาจจะต้องทำ Reranking, เลือกมาสัก 3-5 อัน
	- 03 Not in context
		- เอกสารถูกค้นมาให้แล้วไม่ถูกนำไปใส่ใน context เพื่อเอาไปส่งให้ LLM ต่อ 
		- เกิดจากเอกสารเยอะเกินไป เพราะ Token ถูกจำกัด
	- 04 Not Extracted
		- ข้อมูลที่ต้องการมีและได้มีการส่งให้ LLM ด้วยแต่ว่ามันดัน Extracted ออกมาแล้วไม่เจอข้อมูลที่เราต้องการ
		- เกิดจาก context มี noise เยอะเกินไปหรือข้อมูลขัดแย้งกันเอง -> LLM ตอบผิดพลาด
	- 05 Wrong format
		- เราอยากได้ Field JSON แต่ปรากฏว่ามันไม่เหมือนเดิม (Field reference )
		- เราจะต้องระบุให้ดี
	- 06 Incorrect specificity
		- คำตอบที่ได้กว้างหรือละเอียดเกินไปจนไม่ตอบสนองความต้องการของ user 
	- 07 Incomplete
		- ตอบถูกบ้างแต่ตอบมาไม่หมดตอบมาไม่ครบ
		- อย่างเช่นตอบคำถาม ABC แต่ว่าดันตอบคำถามมาแค่ส่วน AB ไม่มี C
		- อาจจะมีการทำ Evaluate เพื่อป้องกันการหลุดของข้อมูล
![[Pasted image 20250917133940.png]]
- เหตุผลที่มีการใช้ Cloud Database เพราะ มันจะเป็นการลดความ bias ของเครื่องได้
- การใช้ Local storage มันดีกว่าไม่ว่าจะเป็นในเรื่องของ
	- การ Implement
	- Cost
- การใช้แบบ Cloud จะดีกว่าในเรื่องของการ scale
- Pinecone มันเป็น Realtime response, แม่นยำ, ไว, scale ง่าย
- ==ในระดับ Production ส่วนมากจะใช้เป็นรูปแบบ Cloud==
- การทำ Local Vector Database มันจำเป็นจะต้องใช้คอมที่แรงระดับหนึ่ง
- เราสามารถ Local Vector database ได้เชื่อม Ollama
![[Pasted image 20250917134013.png]] 
- Database workflow
	1. Create an index
		- การจัดระเบียบข้อมูล Vector ให้ค้นหาได้เร็วขึ้น
		- มีหลายแบบเช่น diant index, semantic search, spark index, hybrid search
	2. Upsert text
		- Update + Insert = Upsert
		- ถ้าหากข้อมูลนั้นมีอยู่แล้วก็จะเป็นการ Update แต่ว่าถ้าหากไม่มีข้อมูลมันจะโดนแปลงเป็น vector และใส่เข้าไป
		- บางครั้งมันมีข้อมูลเดิมอยู่แล้ว ถ้าโดยปกติมันจะต้องทำการแปลงข้อมูลทั้งหมดแต่ว่าตัวนี้มันจะช่วยความสะดวกของเราทำให้มัน Embedding เฉพาะในส่วนของส่วนที่ยังไม่ได้ทำการ Embedding
	3. Search with text
		- การค้นหาด้วยข้อความ เวลาที่เราต้องการที่จะสืบค้น
		- มันจะใช้ Model ของมันผสม Index ในการทำ Search ต่างๆ
		- นำ Vector ที่ User ใส่เข้ามาไปเปรียบเทียบเอาที่ใกล้เคียงที่สุด
	4. Improve relevance
		- เป็นการทำให้ ผลลัพธ์ออกมาดีที่สุด
		- Filter by metadata เป็นการจำกัดขอบเขตการค้นหาให้แคบลง
		- Rerank result เป็นการเพิ่มความแม่นยำ
		- Lexical search เป็นการค้นหาเชิง Keyword 
![[Pasted image 20250917134714.png]]![[Pasted image 20250917134726.png]]
- "two dogs running" กับ ภาพ เมื่อมีการ Embedding แล้วมันควรที่จะเป็นจุดที่มีความใกล้เคียงกันอยู่บน Vector plane 3D
- 39:38
![[Pasted image 20250925154718.png]]
48:39
- Guardrails มันมีหลาย Service
- AWS ก็มีของเขาเอง
- มันเพื่อป้องกัน Out of domain service
- การมี Guardrails ถ้าหากว่า User ใส่อะไรเข้าไปแล้วติดตรงนี้มันจะเด็งออกเลยโดยที่ไม่เสียเวลาเข้าไป search
- นอกจากนี้ Guardrails สามารถเอาไปเป็น
	- ตัว Activate function
	- router  
![[Pasted image 20250925160420.png]]
- Colang : เป็นภาษาตัวหนึ่งที่ออกแบบมาเพื่อ NVIDIA สำหรับป้องกันบทสนทนาที่มี LLM อยู่ด้านใน
- ถ้ามาดูมันจะมีความคล้ายกับภาษาพูดของคน เหมือนคุย สั่งคน
![[Pasted image 20250918125214.png]]
- ตัวอย่าง application
- OpenAI มีประสิทธิภาพค่อนข้างดีแต่ว่ามันแพง 
![[Pasted image 20250918130050.png]]
![[Pasted image 20250925161551.png]]
- VisionLLM ณ ปัจจุบันมันเก่งมากเราสามารถที่จะใส่รูปหรืออะไรก็ตามเข้าไปได้ Multimodal ปัจจุบันเก่งขึ้นมากๆ ปกติต้องมีการทำ OCR ทำ Parser  แต่เดิมวิธีการทำจะเป็นแบบบนแต่ตอนนี้กลายเป็นแบบล่างไปแล้ว
- แต่ปัจจุบันแทบจะไม่ต้อง ==ประสิทธิภาพดีกว่า== แต่ค่าใช้จ่ายก็อาจจะสูงกว่า
- Process    
	- วิธีเก่า
		- PDF Parser with captioner
			- แปลงข้อมูลที่อยู่ในรูป Plain text ธรรมดาให้สามารถเข้าใช้และทำงานได้ด้วย Computer
			- ข้อเสียคือเวลาทำ PDF parser หรือ OCR เนี้ยข้อมูลที่ได้มันไม่ได้ 100% อาจจะมีหายตกหล่นไปบ้าง ทำให้ขาดบริบทที่สำคัญ
		- OCR
			- OCR จะทำการแปลงรูปภาพไปเป็นตัวอักษร
			- พวกรูปภาพและตารางคือสิ่งที่มักจะมีปัญหา
		- Parsed Doc
			- เอกสารที่อยู่ในรูปที่พร้อมใช้งาน
			- อาจจะอยู่ในรูปแบบ JSON
		- Retrieval model
			- ค้นหาส่วนของข้อมูลที่มีความเกี่ยวค่องที่สุดอาจจะเป็นชิ้นส่วน
			- แล้วส่งไปให้ LLM ตอบ
		- Reranker model
			- เป็นการนำคำตอบที่ได้ไปจัดลำดับใหม่
		- Most Relevant Paragraph
			- หน้าที่เกี่ยวข้องที่สุด เราก็จะมาพิจารณาว่าอะไรเกี่ยวค้องกับคำถามมากที่สุดเวลาที่มีคำถามเข้ามา
	- วิธีใหม่
		- Stack of Documents
		- ColQwen2, DSE + reranker
			- มันสามารถค้นเอกสารที่เกี่ยวข้องได้โดยที่ไม่ต้องผ่านกระบวนการ Parser เลย
			- มันสามารถที่จะ Embedded ข้อมูลทั้งเอกสารและรูปภาพให้มันอยู่ใน vector space เดียวกันได้
			- มันเป็น Multiembedding
			- สามารถวัดความคล้ายคลึงของเอกสารที่ USER ส่งมาได้โดยตรง
			- มันสามารถเข้าใจโครงสร้างเอกสารที่ซับซ่อนรวมถึงเข้าใจภาพได้ดี
			- ข้ามขั้นตอน Parser ไปเพื่อลดความเสี่ยงในการสูญเสียข้อมุล
		- Retrieved document with answer to query
		- VisionLM
			- มันคือ Multimodal LLM มันสามารถประมวลผลได้ทั้ง
				- ข้อความ
				- แผนภูมิ
				- รูปภาพ
			- ทำให้ Model เข้าใจบริบททั้งหมดของเอกสาร
			- ใช้เอกสารที่ค้นมาที่อยู่ในรูปแบบเดิมเพื่อสร้างคำตอบผ่านตัวนี้
- 01:07:24

![[Pasted image 20250919104655.png]]
- ถ้ายังใช้ Cloud -> Groq
- ถ้าอยาก Local  -> Ollama
- Cloud -> Scale, Speed, Effician 
- Local  -> Cost]]


![[05.png]]
- การทำ chunking สามารถทำได้หลายแบบปกติจะใช้วิธี limit chunking ซึ่งมันจะทำให้ข้อมูลเนี้ยถูกตัดไปบางส่วนทำให้มันได้ข้อมูลที่ไม่ครบ
- เราสามารถแบ่ง chunk หลายๆส่วนแล้วเอามา Combine ที่หลังวิธีนี้จะทำให้ข้อมูลที่ได้ทั้งหมดไม่สูญหาย