{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5jmmr7f5kRE"
      },
      "source": [
        "# Inference LLMs RAG, Interface chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwK26y8U5oSd",
        "outputId": "d7421ffe-9aa8-4fdb-dee5-4223d29f1c8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nemoguardrails 0.16.0 requires langchain<0.4.0,>=0.2.14, but you have langchain 0.0.354 which is incompatible.\n",
            "nemoguardrails 0.16.0 requires langchain-community<0.4.0,>=0.2.5, but you have langchain-community 0.0.20 which is incompatible.\n",
            "nemoguardrails 0.16.0 requires langchain-core<0.4.0,>=0.2.14, but you have langchain-core 0.1.23 which is incompatible.\n",
            "pinecone-plugin-assistant 1.8.0 requires packaging<25.0,>=24.2, but you have packaging 23.2 which is incompatible.\n",
            "langchain-pinecone 0.2.12 requires langchain-core<1.0.0,>=0.3.34, but you have langchain-core 0.1.23 which is incompatible.\n",
            "langchain-groq 0.3.8 requires langchain-core<1.0.0,>=0.3.75, but you have langchain-core 0.1.23 which is incompatible.\n",
            "langchain-openai 0.3.33 requires langchain-core<1.0.0,>=0.3.76, but you have langchain-core 0.1.23 which is incompatible.\n",
            "langchain-openai 0.3.33 requires openai<2.0.0,>=1.104.2, but you have openai 1.6.1 which is incompatible.\n",
            "langchain-openai 0.3.33 requires tiktoken<1,>=0.7, but you have tiktoken 0.5.2 which is incompatible.\n",
            "langchain-huggingface 0.3.1 requires langchain-core<1.0.0,>=0.3.70, but you have langchain-core 0.1.23 which is incompatible.\n",
            "xarray 2025.9.0 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\n",
            "google-cloud-bigquery 3.38.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "db-dtypes 1.4.3 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "langchain-text-splitters 0.3.11 requires langchain-core<2.0.0,>=0.3.75, but you have langchain-core 0.1.23 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "    langchain==0.0.354\\\n",
        "    openai==1.6.1\\\n",
        "    pinecone-client==3.1.0\\\n",
        "    tiktoken==0.5.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ytxyw4u3qnTq",
        "outputId": "2b8c8e59-9a63-4fbe-ddb0-b268da158c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nemoguardrails 0.16.0 requires langchain<0.4.0,>=0.2.14, but you have langchain 0.0.354 which is incompatible.\n",
            "nemoguardrails 0.16.0 requires langchain-community<0.4.0,>=0.2.5, but you have langchain-community 0.0.20 which is incompatible.\n",
            "langchain 0.0.354 requires langchain-core<0.2,>=0.1.5, but you have langchain-core 0.3.76 which is incompatible.\n",
            "langchain 0.0.354 requires langsmith<0.1.0,>=0.0.77, but you have langsmith 0.4.30 which is incompatible.\n",
            "langchain-openai 0.3.33 requires openai<2.0.0,>=1.104.2, but you have openai 1.6.1 which is incompatible.\n",
            "langchain-openai 0.3.33 requires tiktoken<1,>=0.7, but you have tiktoken 0.5.2 which is incompatible.\n",
            "langchain-community 0.0.20 requires langchain-core<0.2,>=0.1.21, but you have langchain-core 0.3.76 which is incompatible.\n",
            "langchain-community 0.0.20 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.4.30 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-huggingface\n",
        "!pip install -qU langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GrreK_hkqotV"
      },
      "outputs": [],
      "source": [
        "!pip install -qU nemoguardrails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4Y9DhoJRsOat"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \"langchain[groq]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHPw5m_8r8b2"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LjtHhZ4H9KEQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "groq_api = userdata.get('GROQ_API_USPLACES')\n",
        "# groq_api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cKQwICtVqop3"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "os.environ['GROQ_API_KEY'] = groq_api.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "97hZQX29qona"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToyqTkGusQ0X"
      },
      "outputs": [],
      "source": [
        "# chat = ChatGroq(\n",
        "#     temperature = 0,\n",
        "#     model_name = \"qwen-qwq32b\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat = ChatGroq(\n",
        "    temperature = 0,\n",
        "    model_name = \"llama-3.3-70b-versatile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chQemyhhsQx5",
        "outputId": "aa2d3d5f-4847-42a3-9448-6103dbfed01f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Low latency Large Language Models (LLMs) are crucial for various applications, particularly those that require real-time or near-real-time interactions. Latency refers to the time it takes for a model to process input and generate output. Here are some reasons why low latency LLMs are important:\\n\\n1. **Improved User Experience**: Low latency LLMs enable faster response times, which is essential for applications like chatbots, virtual assistants, and conversational AI. Users expect quick and timely responses, and high latency can lead to frustration and abandonment.\\n2. **Real-time Applications**: Low latency LLMs are necessary for real-time applications, such as:\\n\\t* Live language translation\\n\\t* Real-time text summarization\\n\\t* Live chat support\\n\\t* Virtual event moderation\\n3. **Edge Computing**: With the increasing adoption of edge computing, low latency LLMs can be deployed on edge devices, reducing the need for cloud connectivity and enabling faster processing. This is particularly important for applications like autonomous vehicles, smart homes, and industrial automation.\\n4. **Conversational Flow**: Low latency LLMs help maintain conversational flow, allowing for more natural and engaging interactions. High latency can disrupt the conversation, making it feel stilted or robotic.\\n5. **Competitive Advantage**: Organizations that deploy low latency LLMs can gain a competitive advantage by providing faster and more responsive services, which can lead to increased customer satisfaction and loyalty.\\n6. **Resource Efficiency**: Low latency LLMs can be more resource-efficient, as they require less computational power and memory to achieve the same level of performance as higher-latency models. This can lead to cost savings and reduced environmental impact.\\n7. **Scalability**: Low latency LLMs can handle a larger volume of requests and conversations simultaneously, making them more scalable and suitable for large-scale deployments.\\n8. **Enhanced Security**: Low latency LLMs can help detect and respond to security threats in real-time, reducing the risk of data breaches and cyber attacks.\\n\\nTo achieve low latency, LLMs can be optimized using various techniques, such as:\\n\\n1. **Model pruning**: Reducing the model's size and complexity\\n2. **Knowledge distillation**: Transferring knowledge from a larger model to a smaller one\\n3. **Quantization**: Representing model weights and activations using lower-precision data types\\n4. **Parallel processing**: Distributing computations across multiple processing units\\n5. **Specialized hardware**: Utilizing hardware accelerators, such as GPUs or TPUs, designed for AI workloads\\n\\nBy prioritizing low latency, developers and organizations can create more responsive, efficient, and effective LLMs that provide better user experiences and drive business success.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 553, 'prompt_tokens': 52, 'total_tokens': 605, 'completion_time': 1.249041059, 'prompt_time': 0.002513031, 'queue_time': 0.201455433, 'total_time': 1.25155409}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9e1e8f8435', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--df285626-b0ec-48ea-839f-12ccc491844d-0', usage_metadata={'input_tokens': 52, 'output_tokens': 553, 'total_tokens': 605})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "system = \"You are a helpful assistant.\"\n",
        "human  = \"{text}\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\",system),(\"human\",human)])\n",
        "\n",
        "chain  = prompt | chat\n",
        "chain.invoke({\n",
        "    \"text\" : \"Explain the importance of low latency LLMs.\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ceUKq1w5slRJ"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JC5fijJhu150"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    SystemMessage(content = \"You are a helpful assistant.\"),\n",
        "    HumanMessage(content  = \"Hi AI, how are you today?\"),\n",
        "    AIMessage(content = \"I'm great thank you. How can I help you?\"),\n",
        "    HumanMessage(content = \"I'd like to understand string theory.\")     # prompt engineering AI --> human\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey8pAj78slOH"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\") #Embedding text ---> vector\n",
        "# We have to use the same embedding with the one that We create Vector database\n",
        "embed_model = embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CEBQ-yAjslLl"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "pinecone_api = userdata.get('PINECONE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "__WlLB3KAGQn"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "z9MwrmADAMM6"
      },
      "outputs": [],
      "source": [
        "pc = Pinecone(api_key = pinecone_api)\n",
        "index = pc.Index(\"us-places-ragv99\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "waFOfvqYq-fT"
      },
      "outputs": [],
      "source": [
        "def format_rag_results(documents):\n",
        "    \"\"\"\n",
        "    Format RAG results into a clean, structured representation with preserved source URLs.\n",
        "    Args:\n",
        "        documents : List of Document objects from langchain with metadata and page_content\n",
        "    Returns:\n",
        "        str : A formatted string with the structured information\n",
        "    \"\"\"\n",
        "    formatted_results = []\n",
        "    for i, doc in enumerate(documents):\n",
        "        # Extract metadata\n",
        "        metadata = doc.metadata\n",
        "        content = doc.page_content\n",
        "        # Format each document\n",
        "        doc_formatted = f\"# Document {i+1} : {metadata.get('name','Unnamed Document')}/n/n\"\n",
        "        # Basic information section\n",
        "        doc_formatted += \"## Basic Information\\n\"\n",
        "        for key in ['name','location','categories']:\n",
        "            if key in metadata:\n",
        "                value = metadata[key]\n",
        "                if isinstance(value, list):\n",
        "                    value = \", \".join(value)\n",
        "                doc_formatted += f\"- **{key.title()}**: {value}\\n\"\n",
        "        if 'rating' in metadata:\n",
        "            doc_formatted += f\"- **Rating**: {metadata['rating']}/5\\n\"\n",
        "        # Always include the source URL if available\n",
        "        if 'url' in metadata:\n",
        "            doc_formatted += f\"- **Source URL**: {metadata['url']}\\n\"\n",
        "        # Content section\n",
        "        if content:\n",
        "            doc_formatted += f\"\\n## Description\\n{content}\\n\\n\"\n",
        "        # Reviews section if available\n",
        "        if 'reviews' in metadata and metadata['reviews']:\n",
        "            doc_formatted += \"## Reviews\\n\"\n",
        "            for j, review in enumerate(metadata['reviews']):\n",
        "                # Try to extract a title if the review has a comma\n",
        "                if ',' in review:\n",
        "                    parts = review.split(\",\",1)\n",
        "                    title = parts[0].strip()\n",
        "                    review_text = parts[1].strip()\n",
        "                    doc_formatted += f\"### {title}\\n{review_text}\\n\\n\"\n",
        "                else:\n",
        "                    doc_formatted += f\"### Review {j+1}\\n{review}\\n\\n\"\n",
        "        formatted_results.append(doc_formatted)\n",
        "    # Combine all formatted documents\n",
        "    return \"\\n\".join(formatted_results)\n",
        "\n",
        "# Example usage:\n",
        "# formatted_output = format_rag_results(document)\n",
        "# print(formatted_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yxUpcH24sPmB"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from pinecone import Pinecone\n",
        "from google.colab import userdata\n",
        "pinecone_key = pinecone_api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VtXKSN0AsPib"
      },
      "outputs": [],
      "source": [
        "# Your existing code to set up embeddings and Pinecone\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "embed_model = embeddings\n",
        "pc = Pinecone(api_key = pinecone_api)    # pc = Pinecone(api_key=pinecone_key)\n",
        "index = pc.Index(\"us-places-ragv99\")     # index = pc.Index(\"us-places-rag\")\n",
        "text_field = \"text\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cHhwX3M3UI69",
        "outputId": "8e3866a1-f462-496c-ef66-00853cc0ef9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-pinecone in /usr/local/lib/python3.12/dist-packages (0.2.12)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.12/dist-packages (from langchain-pinecone) (0.3.76)\n",
            "Requirement already satisfied: pinecone<8.0.0,>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (7.3.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from langchain-pinecone) (1.26.4)\n",
            "Requirement already satisfied: langchain-openai>=0.3.11 in /usr/local/lib/python3.12/dist-packages (from langchain-pinecone) (0.3.33)\n",
            "Requirement already satisfied: httpx>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from langchain-pinecone) (0.28.1)\n",
            "Requirement already satisfied: simsimd>=5.9.11 in /usr/local/lib/python3.12/dist-packages (from langchain-pinecone) (6.5.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.0->langchain-pinecone) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.0->langchain-pinecone) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.0->langchain-pinecone) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.0->langchain-pinecone) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.0->langchain-pinecone) (0.16.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (0.4.30)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (23.2)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (2.11.9)\n",
            "Collecting openai<2.0.0,>=1.104.2 (from langchain-openai>=0.3.11->langchain-pinecone)\n",
            "  Using cached openai-1.108.2-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai>=0.3.11->langchain-pinecone)\n",
            "  Using cached tiktoken-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.8.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.5.0)\n",
            "Requirement already satisfied: aiohttp>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (3.12.15)\n",
            "Requirement already satisfied: aiohttp-retry<3.0.0,>=2.9.1 in /usr/local/lib/python3.12/dist-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.9.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.20.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (0.25.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai>=0.3.11->langchain-pinecone) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai>=0.3.11->langchain-pinecone) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai>=0.3.11->langchain-pinecone) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai>=0.3.11->langchain-pinecone) (4.67.1)\n",
            "Collecting packaging>=23.2 (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone)\n",
            "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.17.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai>=0.3.11->langchain-pinecone) (2024.11.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (3.4.3)\n",
            "Using cached openai-1.108.2-py3-none-any.whl (948 kB)\n",
            "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
            "Using cached tiktoken-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Installing collected packages: packaging, tiktoken, openai\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.2\n",
            "    Uninstalling packaging-23.2:\n",
            "      Successfully uninstalled packaging-23.2\n",
            "  Attempting uninstall: tiktoken\n",
            "    Found existing installation: tiktoken 0.5.2\n",
            "    Uninstalling tiktoken-0.5.2:\n",
            "      Successfully uninstalled tiktoken-0.5.2\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.6.1\n",
            "    Uninstalling openai-1.6.1:\n",
            "      Successfully uninstalled openai-1.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-1.108.2 packaging-24.2 tiktoken-0.11.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "dcd933af5cd84c649361f9e8d0adfcae",
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install -U langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtMdWnJisPcg",
        "outputId": "59391e3d-4db8-4d66-abb5-7a50dc9effca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/langchain_pinecone/__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
          ]
        }
      ],
      "source": [
        "from pinecone import Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_O6uSfPTFt9o"
      },
      "outputs": [],
      "source": [
        "# vectorstore = Pinecone(index, embed_model.embed_query, text_field)\n",
        "vectorstore = PineconeVectorStore(\n",
        "    index     = index,\n",
        "    embedding = embed_model,\n",
        "    text_key  = \"text\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0aQSKZmpsPQL"
      },
      "outputs": [],
      "source": [
        "def augment_prompt(query:str):\n",
        "    results = vectorstore.similarity_search(query, k=3)\n",
        "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
        "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "    Contexts:\n",
        "    {source_knowledge}\n",
        "\n",
        "    Query: {query}\"\"\"\n",
        "    return augmented_prompt, results     # Return the results as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7qtDD-9YY6sx"
      },
      "outputs": [],
      "source": [
        "# New function to display formatted results alongs with the RAG response\n",
        "def display_rag_results(query):\n",
        "    augmented_prompt, documents = augment_prompt(query)\n",
        "    # Get the LLM response\n",
        "    prompt = HumanMessage(content=augmented_prompt)\n",
        "    messages.append(prompt)\n",
        "    res = chat(messages)\n",
        "    # Format the retrieved documents\n",
        "    formatted_docs = format_rag_results(documents)\n",
        "    print(\"LLM RESPONSE:\")\n",
        "    print(res.content)\n",
        "    print(\"\\nRETRIEVED SOURCES:\")\n",
        "    print(formatted_docs)\n",
        "    return res.content, formatted_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT4x35PsY6qA",
        "outputId": "744c3a0a-0a9c-4983-9add-8ac3aea974eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3030564552.py:7: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  res = chat(messages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM RESPONSE:\n",
            "Based on the provided contexts, it appears that Vizcaya Museum and Gardens is a lavish villa built in 1916 as a winter retreat, inspired by the Italian Renaissance. The museum features original furnishings and artwork, and is surrounded by lush, formal gardens.\n",
            "\n",
            "RETRIEVED SOURCES:\n",
            "# Document 1 : Vizcaya Museum and Gardens/n/n## Basic Information\n",
            "- **Name**: Vizcaya Museum and Gardens\n",
            "- **Location**: Miami\n",
            "- **Categories**: \n",
            "- **Rating**: 4.5/5\n",
            "- **Source URL**: https://www.tripadvisor.com/Attraction_Review-g34438-d130345-Reviews-Vizcaya_Museum_and_Gardens-Miami_Florida.html\n",
            "\n",
            "## Description\n",
            "Built in 1916 as a winter retreat, this lavish villa is a tribute to the Italian Renaissance. The museum contains much of the original furnishings and artwork, and is surrounded by lush, formal gardens.\n",
            "\n",
            "\n",
            "# Document 2 : Bellagio Conservatory & Botanical Garden/n/n## Basic Information\n",
            "- **Name**: Bellagio Conservatory & Botanical Garden\n",
            "- **Location**: Las Vegas\n",
            "- **Categories**: \n",
            "- **Rating**: 4.5/5\n",
            "- **Source URL**: https://www.tripadvisor.com/Attraction_Review-g45963-d625114-Reviews-Bellagio_Conservatory_Botanical_Garden-Las_Vegas_Nevada.html\n",
            "\n",
            "## Description\n",
            "Brilliance abounds inside our breathtaking Conservatory & Botanical Gardens. The attention to detail is astounding. The passionate display of nature in all its awe-evoking glory - quite simply, sensational! Let your imagination wander as you assume a leisurely stroll amongst rare natural finds selected distinctively for Bellagio from all over the world. Admire the essence of every season recreated with exceptionally gorgeous plants, flowers and trees thoughtfully arranged to inspire full splendor\n",
            "\n",
            "\n",
            "# Document 3 : Conservatory of Flowers/n/n## Basic Information\n",
            "- **Name**: Conservatory of Flowers\n",
            "- **Location**: San Francisco\n",
            "- **Categories**: \n",
            "- **Rating**: 4.5/5\n",
            "- **Source URL**: https://www.tripadvisor.com/Attraction_Review-g60713-d117151-Reviews-Conservatory_of_Flowers-San_Francisco_California.html\n",
            "\n",
            "## Description\n",
            "The oldest existing building in Golden Gate Park, this Victorian-era greenhouse showcases a wide variety of tropical and seasonal flowers.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "query = \"What is Vizcaya Museum and Gardens?\"\n",
        "response, formatted_sources = display_rag_results(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lkqBk_D8Y6nH"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EKbMDN_OY6kb"
      },
      "outputs": [],
      "source": [
        "text_field = \"text\"      # the metadata field that contains our text\n",
        "# initialize the vector store object\n",
        "vectorstore = PineconeVectorStore(\n",
        "    index     = index,\n",
        "    embedding = embed_model,\n",
        "    text_key  = \"text\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEUeZy4IY6hH",
        "outputId": "60111de0-4cf4-4e88-873f-ba1ef1e50e6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='Miami_Vizcaya_Museum_and_Gardens_0', metadata={'categories': [], 'location': 'Miami', 'name': 'Vizcaya Museum and Gardens', 'rating': 4.5, 'reviews': [], 'url': 'https://www.tripadvisor.com/Attraction_Review-g34438-d130345-Reviews-Vizcaya_Museum_and_Gardens-Miami_Florida.html'}, page_content='Built in 1916 as a winter retreat, this lavish villa is a tribute to the Italian Renaissance. The museum contains much of the original furnishings and artwork, and is surrounded by lush, formal gardens.')]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"What is Vizcaya Museum and Gardens?\"\n",
        "vectorstore.similarity_search(query, k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "d8YWOO4efFhc"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6RoY50_fMO2",
        "outputId": "76ece2df-0d46-4f74-f243-fce59cf75d08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing query: Tell me about the best vegetarian restaurants in Los Angeles with good reviews.\n",
            "--- LLM RESPONSE ---\n",
            "The provided context does not have the information.\n",
            "\\n--- RETRIEVED SOURCES (REFERENCES) ---\n",
            "# Document 1 : The Original Farmers Market\\n\\n## Basic Information\n",
            "- **Name**: The Original Farmers Market\n",
            "- **Location**: Los Angeles\n",
            "- **Categories**: \n",
            "- **Rating**: 4.5/5\n",
            "- **Source URL**: https://www.tripadvisor.com/Attraction_Review-g32655-d219483-Reviews-The_Original_Farmers_Market-Los_Angeles_California.html\n",
            "\n",
            "## Description\n",
            "The Original Farmers Market has been Los Angeles’ favorite destination since 1934. With more than 100 old-world grocers, an eclectic array of shops and dozens of restaurants serving cuisine from around the globe in an al fresco setting, the Market is a beloved town square for locals and one of the city’s top tourist attractions. Known for its variety of cuisine, the Market offers something for every taste - American, Brazilian, Cajun, Chinese, French, Italian, Middle Eastern and more! Many of the businesses have been family-owned and operated for generations. Recently the Market had added a variety of artisan grocery and specialty food merchants this year including organic, seasonal produce, gourmet pickles, single origin coffee and more! The Market strictly adheres to all Los Angeles County Covid-19 protocols.\n",
            "\n",
            "\\n# Document 2 : The Grove\\n\\n## Basic Information\n",
            "- **Name**: The Grove\n",
            "- **Location**: Los Angeles\n",
            "- **Categories**: \n",
            "- **Rating**: 4.5/5\n",
            "- **Source URL**: https://www.tripadvisor.com/Attraction_Review-g32655-d547175-Reviews-The_Grove-Los_Angeles_California.html\n",
            "\n",
            "## Description\n",
            "This sprawling outdoor shopping hub is one of California’s most popular malls. Before flocking to the stores, board the free double-decker Grove Trolley to The Original Farmers Market where you can browse gourmet groceries and specialty foods. Back at the mall, you'll find a range of fashion-forward brands and pop-up stores. When hungry, choose between classic American grub, Italian dining, or fast food amidst other cuisines. Be sure to stay for the spectacular light show at the Dancing Fountain as well.\n",
            "\n",
            "Visit The Grove as part of a LA highlights tour, or as a self-guided retail therapy. – Tripadvisor\n",
            "\n",
            "\\n# Document 3 : Chinatown\\n\\n## Basic Information\n",
            "- **Name**: Chinatown\n",
            "- **Location**: Los Angeles\n",
            "- **Categories**: \n",
            "- **Rating**: 3.5/5\n",
            "- **Source URL**: https://www.tripadvisor.com/Attraction_Review-g32655-d116599-Reviews-Chinatown-Los_Angeles_California.html\n",
            "\n",
            "## Description\n",
            "Los Angeles Chinatown is home to great food and numerous specialty stores where you can shop for charming souvenirs amid lanterns and neon signs lining the streets. Don’t miss the Chinese-style buildings around the Central Plaza and the ornate East Gate—one of the most famous landmarks of the area. \n",
            "\n",
            "Chinatown hosts major events throughout the year like the Lunar New Year Parade and the Lantern Festival. It's also a stone’s throw away on foot from the Old Plaza and the historic Olvera Street, so you might want to explore the surroundings when you’re done visiting the district. – Tripadvisor\n",
            "\n",
            "\n",
            "\\n==================================================\\n\n"
          ]
        }
      ],
      "source": [
        "def format_rag_results(documents):\n",
        "    \"\"\"\n",
        "    Format RAG results into a clean, structured representation with preserved source URLs.\n",
        "    \"\"\"\n",
        "    formatted_results = []\n",
        "    for i, doc in enumerate(documents):\n",
        "        # Extract metadata\n",
        "        metadata = doc.metadata\n",
        "        content = doc.page_content\n",
        "        # Format each document\n",
        "        doc_formatted = f\"# Document {i+1} : {metadata.get('name','Unnamed Document')}\\\\n\\\\n\"\n",
        "        # Basic information section\n",
        "        doc_formatted += \"## Basic Information\\n\"\n",
        "        for key in ['name', 'location','categories']:\n",
        "            if key in metadata:\n",
        "                value = metadata[key]\n",
        "                if isinstance(value, list):\n",
        "                    value = \", \".join(value)\n",
        "                doc_formatted += f\"- **{key.title()}**: {value}\\n\"\n",
        "        if 'rating' in metadata:\n",
        "            doc_formatted += f\"- **Rating**: {metadata['rating']}/5\\n\"\n",
        "        # Always include the source URL if available\n",
        "        if 'url' in metadata:\n",
        "            doc_formatted += f\"- **Source URL**: {metadata['url']}\\n\"\n",
        "        # Content section\n",
        "        if content:\n",
        "            doc_formatted += f\"\\n## Description\\n{content}\\n\\n\"\n",
        "        # Reviews section if available\n",
        "        if 'reviews' in metadata and metadata['reviews']:\n",
        "            doc_formatted += \"## Reviews\\n\"\n",
        "            for j, review in enumerate(metadata['reviews']):\n",
        "                # Try to extract a title if the review has a comma\n",
        "                if ',' in review:\n",
        "                    parts = review.split(\",\",1)\n",
        "                    title = parts[0].strip()\n",
        "                    review_text = parts[1].strip()\n",
        "                    doc_formatted += f\"### {title}\\n{review_text}\\n\\n\"\n",
        "                else:\n",
        "                    doc_formatted += f\"### Review {j+1}\\n{review}\\n\\n\"\n",
        "        formatted_results.append(doc_formatted)\n",
        "    # Combine all formatted documents\n",
        "    return \"\\\\n\".join(formatted_results)\n",
        "\n",
        "def augment_prompt(query:str):\n",
        "    results = vectorstore.similarity_search(query,k=3)\n",
        "    source_knowledge = \"\\\\n\".join([x.page_content for x in results])\n",
        "    augmented_prompt_text = f\"\"\"Using the contexts below, answer the query. If the context does not contain the answer, say that the provided context does not have the information.\n",
        "\n",
        "    Contexts:\n",
        "    {source_knowledge}\n",
        "\n",
        "    Query: {query}\n",
        "    \"\"\"\n",
        "    return augmented_prompt_text, results\n",
        "\n",
        "\n",
        "# --- Main Interaction Function ---\n",
        "messages_history = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that answers questions based on the provided context. Do not include any of your own thinking process or notes (e.g., <think>...</think> tags) in the final answer. If the context does not contain the answer, state that the provided context does not have the informat.\")\n",
        "]\n",
        "\n",
        "def remove_thinking_block(text:str) -> str:\n",
        "    \"\"\"Removes the <think>...</think> block from the text.\"\"\"\n",
        "    return re.sub(r\"<think>.*?</think>\\s*\", \"\", text, flags=re.DOTALL).strip()\n",
        "\n",
        "def get_groq_response_with_references(user_query:str):\n",
        "    global messages_history\n",
        "    augmented_prompt_text, retrieved_documents = augment_prompt(user_query)\n",
        "    prompt = HumanMessage(content=augmented_prompt_text)\n",
        "    # Use a copy of the history for the current call, and add the new user prompt\n",
        "    current_conversation_messages = messages_history + [prompt]\n",
        "    try:\n",
        "        llm_response_obj = chat.invoke(current_conversation_messages)\n",
        "        llm_content      = llm_response_obj.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling Groq API: {e}\")\n",
        "        return \"Sorry, I encountered an error trying to get a response.\", \"\"\n",
        "\n",
        "    # Remove the <think> block from the LLM's content\n",
        "    cleaned_llm_content = remove_thinking_block(llm_content)\n",
        "\n",
        "    # Update history with the actual user query and the cleaned LLM response for context in future turns\n",
        "    # For simplicity, we're just adding the last user query and response.\n",
        "    # A more sophiticated hitory management might be needed for longer conversations.\n",
        "    messages_history.append(HumanMessage(content=user_query))          # Add user's original query\n",
        "    messages_history.append(AIMessage(content=cleaned_llm_content))    # Add cleaned AI response\n",
        "    # Keep history to a manageable size (e.g., last N interactions)\n",
        "    if len(messages_history) > 7: # System message + 3 paires of Human/AI\n",
        "        messages_history = [messages_history[0]] + messages_history[-6:]\n",
        "    formatted_sources = format_rag_results(retrieved_documents)\n",
        "\n",
        "    print(\"--- LLM RESPONSE ---\")\n",
        "    print(cleaned_llm_content)\n",
        "    print(\"\\\\n--- RETRIEVED SOURCES (REFERENCES) ---\")\n",
        "    print(formatted_sources)\n",
        "\n",
        "    return cleaned_llm_content, formatted_sources\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    # query1 = \"What is Vizcaya Museum and Gardens?\"\n",
        "    # print(f\"Processing query: {query1}\")\n",
        "    # response1, sources1 = get_groq_response_with_references(query1)\n",
        "    # print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
        "\n",
        "    query2 = \"Tell me about the best vegetarian restaurants in Los Angeles with good reviews.\"\n",
        "    print(f\"Processing query: {query2}\")\n",
        "    response2, sources2 = get_groq_response_with_references(query2)\n",
        "    print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
        "\n",
        "    # query3 = \"Any recommendations for art museums in Boston?\"\n",
        "    # print(f\"Processing query: {query3}\")\n",
        "    # response3, sources3 = get_groq_response_with_references(query3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syuUBrjJtQKi",
        "outputId": "7c981dad-4bab-40e1-a323-cd16f0ab58d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing query: \"What is Vizcaya Museum and Gardens?\"\n",
            "----------------------------------------------------------------------\n",
            "--- LLM RESPONSE (Without RAG) ---\n",
            "Vizcaya Museum and Gardens is a National Historic Landmark located in Coconut Grove, Miami, Florida. It was built in the early 1900s as a winter residence for James Deering, a wealthy industrialist. The estate features an impressive European-inspired mansion with 54 rooms, surrounded by 50 acres of beautifully landscaped gardens, including a variety of plants, trees, and sculptures.\n",
            "\n",
            "The mansion was designed by architect F. Burrall Hoffman and decorator Paul Chalfin, and its construction took over 1,000 workers and 4 years to complete. The estate's design was influenced by Mediterranean Revival architecture, with a mix of French, Italian, and Spanish styles.\n",
            "\n",
            "Vizcaya Museum and Gardens is now owned by Miami-Dade County and is open to the public for tours. The estate showcases an extensive collection of European art and furnishings from the 15th to the 19th centuries, as well as a variety of decorative arts, including ceramics, textiles, and sculptures.\n",
            "\n",
            "The gardens are a highlight of the estate, featuring a range of plant species, including orchids, bromeliads, and tropical trees. The gardens also include several walking trails, a mangrove forest, and a variety of sculptures and fountains.\n",
            "\n",
            "Vizcaya Museum and Gardens is a popular tourist destination and a must-see attraction in South Florida, offering a unique glimpse into the region's rich history and cultural heritage. It has also been used as a filming location for several movies and television shows, including \"The Birdcage\" and \"Iron Man 3.\"\n",
            "\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\n",
            "--- LLM RESPONSE (With RAG) ---\n",
            "The provided context does not have the information.\n",
            "\\n--- RETRIEVED SOURCES (REFERENCES FOR RAG) ---\n",
            "# Document 1: Vizcaya Museum and Gardens\\n\\n## Basic Information\\n- **Name**: Vizcaya Museum and Gardens\\n- **Location**: Miami\\n- **Categories**: \\n- **Rating**: 4.5/5\\n- **Source URL**: https://www.tripadvisor.com/Attraction_Review-g34438-d130345-Reviews-Vizcaya_Museum_and_Gardens-Miami_Florida.html\\n\\n## Description\\nBuilt in 1916 as a winter retreat, this lavish villa is a tribute to the Italian Renaissance. The museum contains much of the original furnishings and artwork, and is surrounded by lush, formal gardens.\\n\\n\\n# Document 2: Bellagio Conservatory & Botanical Garden\\n\\n## Basic Information\\n- **Name**: Bellagio Conservatory & Botanical Garden\\n- **Location**: Las Vegas\\n- **Categories**: \\n- **Rating**: 4.5/5\\n- **Source URL**: https://www.tripadvisor.com/Attraction_Review-g45963-d625114-Reviews-Bellagio_Conservatory_Botanical_Garden-Las_Vegas_Nevada.html\\n\\n## Description\\nBrilliance abounds inside our breathtaking Conservatory & Botanical Gardens. The attention to detail is astounding. The passionate display of nature in all its awe-evoking glory - quite simply, sensational! Let your imagination wander as you assume a leisurely stroll amongst rare natural finds selected distinctively for Bellagio from all over the world. Admire the essence of every season recreated with exceptionally gorgeous plants, flowers and trees thoughtfully arranged to inspire full splendor\\n\\n\\n# Document 3: Conservatory of Flowers\\n\\n## Basic Information\\n- **Name**: Conservatory of Flowers\\n- **Location**: San Francisco\\n- **Categories**: \\n- **Rating**: 4.5/5\\n- **Source URL**: https://www.tripadvisor.com/Attraction_Review-g60713-d117151-Reviews-Conservatory_of_Flowers-San_Francisco_California.html\\n\\n## Description\\nThe oldest existing building in Golden Gate Park, this Victorian-era greenhouse showcases a wide variety of tropical and seasonal flowers.\\n\\n\n",
            "\\n======================================================================\\n\n",
            "Processing query: \"Tell me about the best vegetarian restaurants in Los Angeles with good reviews.\"\n",
            "----------------------------------------------------------------------\n",
            "--- LLM RESPONSE (Without RAG) ---\n",
            "Los Angeles is a haven for vegetarian foodies, with a plethora of excellent restaurants to choose from. Here are some of the best vegetarian restaurants in LA with great reviews:\n",
            "\n",
            "1. **Cafe Gratitude**: A fully plant-based restaurant with multiple locations in LA, serving creative and delicious dishes like \"I Am Elated\" (a vegan take on eggs benedict) and \"I Am Abundant\" (a hearty bowl of quinoa and vegetables). Reviewers rave about the friendly service and innovative menu.\n",
            "\n",
            "2. **M Cafe**: A popular spot with several locations in LA, M Cafe offers a variety of vegetarian and vegan options, including a famous vegan \"Big Macro\" burger and a seasonal vegetable quinoa bowl. Reviewers praise the restaurant's cozy atmosphere and generous portions.\n",
            "\n",
            "3. **The Butcher's Daughter**: Although not entirely vegetarian, this charming cafe has a separate vegetarian menu, which changes seasonally. Reviewers love the beautiful presentation and creative dishes like the \"Vegetable Tartine\" and \"Golden Latte.\"\n",
            "\n",
            "4. **Donut Friend**: A vegan donut shop with a fun and playful atmosphere, Donut Friend offers a wide range of creative donut flavors, from classic glazed to more unique options like strawberry basil and chocolate chipotle. Reviewers rave about the delicious donuts and friendly staff.\n",
            "\n",
            "5. **Shojin**: A Japanese-inspired vegan restaurant in Little Tokyo, Shojin offers a variety of small plates, noodle dishes, and sushi options. Reviewers praise the restaurant's cozy atmosphere and creative takes on traditional Japanese dishes.\n",
            "\n",
            "6. **Plant Food + Wine**: A upscale vegetarian restaurant in Venice, Plant Food + Wine offers a seasonal menu of creative, farm-to-table dishes like roasted beet tart and wild mushroom risotto. Reviewers rave about the beautiful presentation and exceptional service.\n",
            "\n",
            "7. **Sweetfin Poké**: A popular poké bowl chain with multiple locations in LA, Sweetfin offers a variety of vegetarian and vegan options, including a vegan \"Spicy Mango\" bowl and a seasonal vegetable bowl. Reviewers praise the fresh ingredients and customizable options.\n",
            "\n",
            "8. **Vegan Glory**: A fully vegan restaurant in West Hollywood, Vegan Glory offers a variety of international dishes, from Italian pasta to Indian curries. Reviewers rave about the generous portions and friendly service.\n",
            "\n",
            "These are just a few of the many excellent vegetarian restaurants in Los Angeles. Whether you're in the mood for creative, farm-to-table cuisine or comforting, indulgent treats, LA has something for every vegetarian foodie.\n",
            "\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\n",
            "--- LLM RESPONSE (With RAG) ---\n",
            "The provided context does not have the information about the best vegetarian restaurants in Los Angeles with good reviews. It provides information about The Original Farmers Market, The Grove, and Los Angeles Chinatown, but does not specifically mention vegetarian restaurants or their reviews.\n",
            "\\n--- RETRIEVED SOURCES (REFERENCES FOR RAG) ---\n",
            "# Document 1: The Original Farmers Market\\n\\n## Basic Information\\n- **Name**: The Original Farmers Market\\n- **Location**: Los Angeles\\n- **Categories**: \\n- **Rating**: 4.5/5\\n- **Source URL**: https://www.tripadvisor.com/Attraction_Review-g32655-d219483-Reviews-The_Original_Farmers_Market-Los_Angeles_California.html\\n\\n## Description\\nThe Original Farmers Market has been Los Angeles’ favorite destination since 1934. With more than 100 old-world grocers, an eclectic array of shops and dozens of restaurants serving cuisine from around the globe in an al fresco setting, the Market is a beloved town square for locals and one of the city’s top tourist attractions. Known for its variety of cuisine, the Market offers something for every taste - American, Brazilian, Cajun, Chinese, French, Italian, Middle Eastern and more! Many of the businesses have been family-owned and operated for generations. Recently the Market had added a variety of artisan grocery and specialty food merchants this year including organic, seasonal produce, gourmet pickles, single origin coffee and more! The Market strictly adheres to all Los Angeles County Covid-19 protocols.\\n\\n\\n# Document 2: The Grove\\n\\n## Basic Information\\n- **Name**: The Grove\\n- **Location**: Los Angeles\\n- **Categories**: \\n- **Rating**: 4.5/5\\n- **Source URL**: https://www.tripadvisor.com/Attraction_Review-g32655-d547175-Reviews-The_Grove-Los_Angeles_California.html\\n\\n## Description\\nThis sprawling outdoor shopping hub is one of California’s most popular malls. Before flocking to the stores, board the free double-decker Grove Trolley to The Original Farmers Market where you can browse gourmet groceries and specialty foods. Back at the mall, you'll find a range of fashion-forward brands and pop-up stores. When hungry, choose between classic American grub, Italian dining, or fast food amidst other cuisines. Be sure to stay for the spectacular light show at the Dancing Fountain as well.\n",
            "\n",
            "Visit The Grove as part of a LA highlights tour, or as a self-guided retail therapy. – Tripadvisor\\n\\n\\n# Document 3: Chinatown\\n\\n## Basic Information\\n- **Name**: Chinatown\\n- **Location**: Los Angeles\\n- **Categories**: \\n- **Rating**: 3.5/5\\n- **Source URL**: https://www.tripadvisor.com/Attraction_Review-g32655-d116599-Reviews-Chinatown-Los_Angeles_California.html\\n\\n## Description\\nLos Angeles Chinatown is home to great food and numerous specialty stores where you can shop for charming souvenirs amid lanterns and neon signs lining the streets. Don’t miss the Chinese-style buildings around the Central Plaza and the ornate East Gate—one of the most famous landmarks of the area. \n",
            "\n",
            "Chinatown hosts major events throughout the year like the Lunar New Year Parade and the Lantern Festival. It's also a stone’s throw away on foot from the Old Plaza and the historic Olvera Street, so you might want to explore the surroundings when you’re done visiting the district. – Tripadvisor\\n\\n\n",
            "\\n======================================================================\\n\n",
            "Processing query: \"Any recommendations for art museums in Boston?\"\n",
            "----------------------------------------------------------------------\n",
            "--- LLM RESPONSE (Without RAG) ---\n",
            "Boston is home to a rich cultural scene, with numerous world-class art museums. Here are some top recommendations:\n",
            "\n",
            "1. Museum of Fine Arts (MFA): One of the largest and most visited art museums in the country, the MFA features a vast collection of over 450,000 works of art, including European, American, and Asian pieces.\n",
            "2. Isabella Stewart Gardner Museum: This unique museum is housed in a stunning Venetian-style palace and showcases an impressive collection of European and American art, as well as rare books and manuscripts.\n",
            "3. Institute of Contemporary Art (ICA): Located on the waterfront, the ICA is one of the oldest modern art museums in the US, featuring an innovative collection of contemporary art, including works by local and international artists.\n",
            "4. Harvard Art Museums: Comprising three separate museums (Fogg, Busch-Reisinger, and Sackler), the Harvard Art Museums offer a diverse range of art and artifacts from around the world, including European and American paintings, sculpture, and decorative arts.\n",
            "5. Boston University Art Gallery: This smaller museum features a diverse range of exhibitions, from contemporary art to historical pieces, and is a great place to discover new artists and styles.\n",
            "\n",
            "These museums offer a great starting point for exploring Boston's vibrant art scene. Be sure to check their websites for current exhibitions, hours, and admission information.\n",
            "\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\n",
            "--- LLM RESPONSE (With RAG) ---\n",
            "Yes, there are several art museum recommendations in Boston. The Museum of Fine Arts (MFA) is Boston's oldest, largest, and best-known art institution, featuring a comprehensive art collection, including Impressionist paintings, Asian and Egyptian collections, and early American art. Another option is the Isabella Stewart Gardner Museum, which is a Venetian palace in the middle of Boston, showcasing an eclectic collection of European, American, and Asian art, including sculpture, paintings, furniture, ceramics, and textiles, with a beautiful skylit courtyard. Additionally, there is a contemporary art museum featuring cutting-edge contemporary painting, sculpture, architecture, film, and photography, although its name is not specified.\n",
            "\\n--- RETRIEVED SOURCES (REFERENCES FOR RAG) ---\n",
            "# Document 1: Museum of Fine Arts\\n\\n## Basic Information\\n- **Name**: Museum of Fine Arts\\n- **Location**: Boston\\n- **Categories**: \\n- **Rating**: 5.0/5\\n- **Source URL**: https://www.tripadvisor.com/Attraction_Review-g60745-d105257-Reviews-Museum_of_Fine_Arts-Boston_Massachusetts.html\\n\\n## Description\\nBoston's oldest, largest and best-known art institution, the MFA houses one of the world's most comprehensive art collections and is renowned for its Impressionist paintings, Asian and Egyptian collections and early American art.\\n\\n\\n# Document 2: The Institute of Contemporary Art\\n\\n## Basic Information\\n- **Name**: The Institute of Contemporary Art\\n- **Location**: Boston\\n- **Categories**: \\n- **Rating**: 3.5/5\\n- **Source URL**: https://www.tripadvisor.com/Attraction_Review-g60745-d133818-Reviews-The_Institute_of_Contemporary_Art-Boston_Massachusetts.html\\n\\n## Description\\nThis museum features cutting-edge contemporary painting, sculpture, architecture, film and photography.\\n\\n\\n# Document 3: Isabella Stewart Gardner Museum\\n\\n## Basic Information\\n- **Name**: Isabella Stewart Gardner Museum\\n- **Location**: Boston\\n- **Categories**: \\n- **Rating**: 4.5/5\\n- **Source URL**: https://www.tripadvisor.com/Attraction_Review-g60745-d108823-Reviews-Isabella_Stewart_Gardner_Museum-Boston_Massachusetts.html\\n\\n## Description\\nA Venetian palace in the middle of Boston, Gardner's home is now a museum displaying her impressive, eclectic collection of European, American and Asian art, including sculpture, paintings, furniture, ceramics and textiles. Visitors can stroll or rest in a spectacular skylit courtyard filled with plants and flowers.\\n\\n\n",
            "\\n======================================================================\\n\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def remove_thinking_block(text: str) -> str:\n",
        "    \"\"\"Removes the <think>...</think> block from the text.\"\"\"\n",
        "    return re.sub(r\"<think>.*?</think>\\s*\", \"\", text, flags=re.DOTALL).strip()\n",
        "\n",
        "def format_rag_results(documents):\n",
        "    \"\"\"\n",
        "    Format RAG results into a clean, structured representation with preserved source URLs.\n",
        "    \"\"\"\n",
        "    formatted_results = []\n",
        "    for i, doc in enumerate(documents):\n",
        "        metadata = doc.metadata\n",
        "        content = doc.page_content\n",
        "        doc_formatted = f\"# Document {i+1}: {metadata.get('name', 'Unnamed Document')}\\\\n\\\\n\"\n",
        "        doc_formatted += \"## Basic Information\\\\n\"\n",
        "        for key in ['name', 'location', 'categories']:\n",
        "            if key in metadata:\n",
        "                value = metadata[key]\n",
        "                if isinstance(value, list):\n",
        "                    value = \", \".join(value)\n",
        "                doc_formatted += f\"- **{key.title()}**: {value}\\\\n\"\n",
        "        if 'rating' in metadata:\n",
        "            doc_formatted += f\"- **Rating**: {metadata['rating']}/5\\\\n\"\n",
        "        if 'url' in metadata:\n",
        "            doc_formatted += f\"- **Source URL**: {metadata['url']}\\\\n\"\n",
        "        if content:\n",
        "            doc_formatted += f\"\\\\n## Description\\\\n{content}\\\\n\\\\n\"\n",
        "        if 'reviews' in metadata and metadata['reviews']:\n",
        "            doc_formatted += \"## Reviews\\\\n\"\n",
        "            for j, review in enumerate(metadata['reviews']):\n",
        "                if ',' in review: # Simple check if review might have a title-like part\n",
        "                    parts = review.split(',', 1)\n",
        "                    title = parts[0].strip()\n",
        "                    review_text = parts[1].strip()\n",
        "                    doc_formatted += f\"### {title}\\\\n{review_text}\\\\n\\\\n\"\n",
        "                else:\n",
        "                    doc_formatted += f\"### Review {j+1}\\\\n{review}\\\\n\\\\n\"\n",
        "        formatted_results.append(doc_formatted)\n",
        "    return \"\\\\n\".join(formatted_results)\n",
        "\n",
        "# --- RAG Functions ---\n",
        "\n",
        "def augment_prompt(query: str):\n",
        "    results = vectorstore.similarity_search(query, k=3)\n",
        "    source_knowledge = \"\\\\n\".join([x.page_content for x in results])\n",
        "    augmented_prompt_text = f\"\"\"Using the contexts below, answer the query. If the context does not contain the answer, say that the provided context does not have the information.\n",
        "\n",
        "    Contexts:\n",
        "    {source_knowledge}\n",
        "\n",
        "    Query: {query}\"\"\"\n",
        "    return augmented_prompt_text, results\n",
        "\n",
        "# Global message history for RAG (can be adjusted)\n",
        "rag_messages_history = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that answers questions based on the provided context. Do not include any of your own thinking process or notes (e.g., <think>...</think> tags) in the final answer. If the context does not contain the answer, state that the provided context does not have the information.\")\n",
        "]\n",
        "\n",
        "def get_llm_response_with_rag(user_query: str):\n",
        "    global rag_messages_history\n",
        "    augmented_prompt_text, retrieved_documents = augment_prompt(user_query)\n",
        "    prompt = HumanMessage(content=augmented_prompt_text)\n",
        "    current_conversation_messages = rag_messages_history + [prompt]\n",
        "\n",
        "    try:\n",
        "        llm_response_obj = chat.invoke(current_conversation_messages)\n",
        "        llm_content = llm_response_obj.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling Groq API (with RAG): {e}\")\n",
        "        return \"Sorry, I encountered an error trying to get a RAG response.\", \"\"\n",
        "\n",
        "    cleaned_llm_content = remove_thinking_block(llm_content)\n",
        "\n",
        "    # Update RAG history (optional, for conversational context)\n",
        "    # rag_messages_history.append(HumanMessage(content=user_query)) # Using augmented prompt might be too verbose for history\n",
        "    # rag_messages_history.append(AIMessage(content=cleaned_llm_content))\n",
        "    # if len(rag_messages_history) > 7:\n",
        "    #     rag_messages_history = [rag_messages_history[0]] + rag_messages_history[-6:]\n",
        "\n",
        "    formatted_sources = format_rag_results(retrieved_documents)\n",
        "    return cleaned_llm_content, formatted_sources\n",
        "\n",
        "# --- Non-RAG Function ---\n",
        "non_rag_messages_history = [\n",
        "    SystemMessage(content=\"You are a helpful assistant. Answer the user's query based on your general knowledge. Do not include any of your own thinking process or notes (e.g., <think>...</think> tags) in the final answer.\")\n",
        "]\n",
        "\n",
        "def get_llm_response_without_rag(user_query: str):\n",
        "    global non_rag_messages_history\n",
        "    prompt = HumanMessage(content=user_query)\n",
        "    current_conversation_messages = non_rag_messages_history + [prompt]\n",
        "\n",
        "    try:\n",
        "        llm_response_obj = chat.invoke(current_conversation_messages)\n",
        "        llm_content = llm_response_obj.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling Groq API (without RAG): {e}\")\n",
        "        return \"Sorry, I encountered an error trying to get a non-RAG response.\"\n",
        "\n",
        "    cleaned_llm_content = remove_thinking_block(llm_content)\n",
        "\n",
        "    # Update Non-RAG history (optional)\n",
        "    # non_rag_messages_history.append(prompt)\n",
        "    # non_rag_messages_history.append(AIMessage(content=cleaned_llm_content))\n",
        "    # if len(non_rag_messages_history) > 7:\n",
        "    #    non_rag_messages_history = [non_rag_messages_history[0]] + non_rag_messages_history[-6:]\n",
        "\n",
        "    return cleaned_llm_content\n",
        "\n",
        "# --- Main Execution for Comparison ---\n",
        "if __name__ == \"__main__\":\n",
        "    queries_to_test = [\n",
        "        \"What is Vizcaya Museum and Gardens?\",\n",
        "        \"Tell me about the best vegetarian restaurants in Los Angeles with good reviews.\",\n",
        "        \"Any recommendations for art museums in Boston?\"\n",
        "    ]\n",
        "\n",
        "    for query in queries_to_test:\n",
        "        print(f\"Processing query: \\\"{query}\\\"\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        # 1. Get response WITHOUT RAG\n",
        "        print(\"--- LLM RESPONSE (Without RAG) ---\")\n",
        "        response_no_rag = get_llm_response_without_rag(query)\n",
        "        print(response_no_rag)\n",
        "        print(\"\\\\n\" + \"~\"*70 + \"\\\\n\")\n",
        "\n",
        "        # 2. Get response WITH RAG\n",
        "        print(\"--- LLM RESPONSE (With RAG) ---\")\n",
        "        response_with_rag, sources = get_llm_response_with_rag(query)\n",
        "        print(response_with_rag)\n",
        "        print(\"\\\\n--- RETRIEVED SOURCES (REFERENCES FOR RAG) ---\")\n",
        "        print(sources)\n",
        "        print(\"\\\\n\" + \"=\"*70 + \"\\\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Colang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fBlN7Tf7BFY",
        "outputId": "8f3a4353-ede5-48dd-c1ce-0289fedc6baa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.py created successfully.\n"
          ]
        }
      ],
      "source": [
        "config_py_content = \"\"\"\n",
        "from langchain_groq import ChatGroq\n",
        "from nemoguardrails.llm.providers import register_llm_provider\n",
        "\n",
        "# Register Groq as a custom LLM provider\n",
        "register_llm_provider(\"groq\", ChatGroq)\n",
        "\"\"\"\n",
        "\n",
        "with open(\"config.py\",\"w\") as f:\n",
        "    f.write(config_py_content)\n",
        "\n",
        "print(\"config.py created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- เราสามารถที่จะใช้ Guardrail เป็นตัวที่ส่งข้อมูลไปให้อีกฟังก์ชันได้ด้วย"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "f3ut23-j7uK7"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import HumanMessage, SystemMessage # Ensure SystemMessage is imported\n",
        "\n",
        "# Make sure your other necessary functions and variables are defined and accessible:\n",
        "# - chat: Your initialized ChatGroq model\n",
        "# - augment_prompt(user_query: str) -> (str, list_of_documents):\n",
        "#   This function should return the augmented prompt string for the LLM\n",
        "#   and the list of raw retrieved document objects.\n",
        "# - remove_thinking_block(text: str) -> str: Your function to clean LLM output.\n",
        "# - format_rag_results(documents: list_of_documents) -> str:\n",
        "#   Your existing function that creates a detailed, formatted string of the RAG sources.\n",
        "#   We will also add a more concise formatting for direct inclusion in the bot response.\n",
        "\n",
        "def rag_query_action(user_query: str):\n",
        "    \"\"\"\n",
        "    This function is registered as an action with Nemo Guardrails.\n",
        "    It performs the RAG process, aims for a more detailed answer, and includes sources.\n",
        "    \"\"\"\n",
        "    print(f\"\\n[Nemo Action LOG] Received query for RAG: {user_query}\")\n",
        "\n",
        "    # 1. Augment the prompt and get retrieved documents\n",
        "    # This function (augment_prompt) is from your notebook.\n",
        "    # It constructs the prompt that includes the retrieved context and the original query.\n",
        "    augmented_prompt_text, retrieved_documents = augment_prompt(user_query)\n",
        "\n",
        "    # 2. Prepare messages for the LLM call\n",
        "    # We'll add a SystemMessage to encourage more detailed and synthesized answers.\n",
        "    # The augmented_prompt_text itself (from your augment_prompt function) already\n",
        "    # contains the core instruction, context, and query.\n",
        "    rag_call_messages = [\n",
        "        SystemMessage(content=(\n",
        "            \"You are a helpful and informative assistant. Based on the provided 'Contexts', \"\n",
        "            \"please provide a comprehensive and detailed answer to the 'Query'. \"\n",
        "            \"Synthesize the information from all relevant parts of the contexts to create a cohesive response. \"\n",
        "            \"If the contexts describe a place or thing that matches the query's subject, even if the exact name isn't repeated in the context, try to provide the relevant information. \"\n",
        "            \"If the contexts truly do not contain relevant information to answer the query, clearly state that.\"\n",
        "        )),\n",
        "        HumanMessage(content=augmented_prompt_text) # This contains: \"Contexts: ...\" and \"Query: ...\"\n",
        "    ]\n",
        "\n",
        "    print(f\"[Nemo Action LOG] Augmented prompt text being sent to LLM (first 200 chars): {augmented_prompt_text[:200]}...\")\n",
        "\n",
        "    # 3. Call the LLM\n",
        "    try:\n",
        "        llm_response_obj = chat.invoke(rag_call_messages)\n",
        "        llm_content = llm_response_obj.content\n",
        "    except Exception as e:\n",
        "        print(f\"[Nemo Action ERROR] Error calling LLM for RAG: {e}\")\n",
        "        # Fallback message if the LLM call fails\n",
        "        return \"My apologies, I encountered an issue while trying to retrieve detailed information. Please try again later.\"\n",
        "\n",
        "    # 4. Clean the LLM's response\n",
        "    cleaned_llm_content = remove_thinking_block(llm_content)\n",
        "\n",
        "    # 5. Format and include references/sources\n",
        "    formatted_sources_text = \"\"\n",
        "    if retrieved_documents:\n",
        "        # Option A: Using your existing detailed format_rag_results (if you want a very verbose source list)\n",
        "        # This might be too long for a single bot utterance.\n",
        "        # detailed_sources = format_rag_results(retrieved_documents)\n",
        "        # formatted_sources_text = f\"\\n\\n--- Details from Retrieved Context ---\\n{detailed_sources}\"\n",
        "\n",
        "        # Option B: A more concise list of sources for the bot's response\n",
        "        concise_sources_list = []\n",
        "        for i, doc in enumerate(retrieved_documents):\n",
        "            # Prefer 'name' metadata for the source title\n",
        "            source_title = doc.metadata.get('name', f\"Retrieved Context {i+1}\")\n",
        "            # Include URL if available\n",
        "            source_url = doc.metadata.get('url', '')\n",
        "            source_entry = f\"- {source_title}\"\n",
        "            if source_url:\n",
        "                source_entry += f\" (Source: {source_url})\"\n",
        "            concise_sources_list.append(source_entry)\n",
        "\n",
        "        if concise_sources_list:\n",
        "            formatted_sources_text = \"\\n\\n**Here are some sources I used:**\\n\" + \"\\n\".join(concise_sources_list)\n",
        "    else:\n",
        "        formatted_sources_text = \"\\n\\n(No specific documents were retrieved to answer this query.)\"\n",
        "\n",
        "    # 6. Combine the LLM's answer with the formatted sources\n",
        "    final_response_with_sources = f\"{cleaned_llm_content}{formatted_sources_text}\"\n",
        "\n",
        "    print(f\"[Nemo Action LOG] Cleaned LLM Content: {cleaned_llm_content}\")\n",
        "    if retrieved_documents:\n",
        "        print(f\"[Nemo Action LOG] Concise Sources: {formatted_sources_text}\")\n",
        "    print(f\"[Nemo Action LOG] Final response being returned to Guardrails (first 300 chars): {final_response_with_sources[:300]}...\")\n",
        "\n",
        "    return final_response_with_sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2PI7pbMJ7u2E"
      },
      "outputs": [],
      "source": [
        "yaml_content_for_rag = \"\"\"\n",
        "models:\n",
        "- type: main\n",
        "  engine: groq\n",
        "  model: llama3-70b-8192 # Or your preferred Groq model\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- อันนี้เป็น Gardrails มันจะมีลักษณะเหมือน Prompt engineer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "UqP_7ziK7uzt"
      },
      "outputs": [],
      "source": [
        "colang_content_for_rag = \"\"\"\n",
        "# Define a custom action for RAG queries\n",
        "# The name \"rag_query_action\" must match the name used when registering the Python function.\n",
        "\n",
        "# --- Greetings and Basic Interaction ---\n",
        "define user express greeting\n",
        "    \"hello\"\n",
        "    \"hi\"\n",
        "    \"hey there\"\n",
        "\n",
        "define bot express greeting\n",
        "    \"Hello! I'm your US travel assistant. How can I help you today?\"\n",
        "\n",
        "define flow greeting\n",
        "    user express greeting\n",
        "    bot express greeting\n",
        "    bot offer further assistance\n",
        "\n",
        "# --- RAG-Specific Flows ---\n",
        "define user ask about_us_place\n",
        "    # Example intents that should trigger the RAG action\n",
        "    \"What is Vizcaya Museum and Gardens?\"\n",
        "    \"Tell me about the Original Farmers Market in Los Angeles.\"\n",
        "    \"Any recommendations for art museums in Boston?\"\n",
        "    \"What can you tell me about {entity} in {location}?\"\n",
        "    \"Describe {place_name}.\"\n",
        "\n",
        "define flow handle_us_place_query_with_rag\n",
        "    user ask about_us_place\n",
        "    bot inform \"Let me check my information about that...\"\n",
        "    # Execute the custom Python action for RAG\n",
        "    $rag_answer = execute rag_query_action(user_query=$last_user_message)\n",
        "    bot $rag_answer\n",
        "    bot offer further assistance\n",
        "\n",
        "# --- Standard Guardrails (e.g., for out-of-scope topics) ---\n",
        "define user ask politics\n",
        "    \"what are your political beliefs?\"\n",
        "    \"thoughts on the president?\"\n",
        "    \"left wing\"\n",
        "    \"right wing\"\n",
        "\n",
        "define bot refuse politics\n",
        "    \"As a US travel assistant, I focus on providing information about places and travel. I don't discuss politics.\"\n",
        "\n",
        "define flow politics\n",
        "    user ask politics\n",
        "    bot refuse politics\n",
        "    bot offer further assistance\n",
        "\n",
        "# --- Fallback for unhandled queries ---\n",
        "# You might want this to also try the RAG, or give a \"cannot help\" message\n",
        "define flow unhandled_query_fallback\n",
        "    user ...\n",
        "    # For this example, let's try the RAG as a general fallback if appropriate\n",
        "    # Or, you could have a simpler LLM call here if RAG is too resource-intensive for all fallbacks\n",
        "    bot inform \"I'll try to find information on that using my knowledge base.\"\n",
        "    $fallback_rag_answer = execute rag_query_action(user_query=$last_user_message)\n",
        "    bot $fallback_rag_answer\n",
        "    bot offer further assistance\n",
        "\n",
        "# --- Reusable bot utterances ---\n",
        "define bot offer further assistance\n",
        "    \"Is there anything else I can help you with regarding US travel or specific places?\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "UmSAp9nY7uwq"
      },
      "outputs": [],
      "source": [
        "# from langchain_groq import ChatGroq\n",
        "# from nemoguardrails.llm.providers import register_llm_provider\n",
        "\n",
        "# register_llm_provider(\"groq\", ChatGroq)\n",
        "!pip install -qU nemoguardrails langchain-groq\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from nemoguardrails.llm.helpers import get_llm_instance_wrapper\n",
        "from nemoguardrails.llm.providers import register_llm_provider\n",
        "\n",
        "# 1) Make a LangChain instance\n",
        "chat = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0)\n",
        "\n",
        "# 2) Wrap it into a provider class Guardrails understands\n",
        "GroqLCWrapped = get_llm_instance_wrapper(llm_instance=chat, llm_type=\"groq\")\n",
        "\n",
        "# 3) Register the wrapped class\n",
        "register_llm_provider(\"groq\", GroqLCWrapped)\n",
        "\n",
        "# 4) Use engine: groq in YAML\n",
        "yaml_content_for_rag = \"\"\"\n",
        "models:\n",
        "- type: main\n",
        "  engine: groq\n",
        "  model: llama-3.3-70b-versatile\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv73Wzfx7ut_",
        "outputId": "08bec19d-6a24-4b53-fd63-57482862c9cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nemo LLMRails initialized with Groq and custom RAG action ('rag_query_action') registered.\n"
          ]
        }
      ],
      "source": [
        "from nemoguardrails import LLMRails, RailsConfig\n",
        "# Ensure 'chat' (Groq LLM) and RAG helper functions are defined and available in this scope.\n",
        "# Also, 'config_py_content' should have been written to config.py\n",
        "\n",
        "# 1. Initialize RailsConfig\n",
        "# This will automatically look for 'config.py' if it's in the CWD or specified path.\n",
        "config = RailsConfig.from_content(\n",
        "    colang_content=colang_content_for_rag,  # Use the new Colang content\n",
        "    yaml_content=yaml_content_for_rag      # Use the YAML for Groq\n",
        ")\n",
        "\n",
        "# 2. Create LLMRails instance\n",
        "rails = LLMRails(config)\n",
        "\n",
        "# 3. Register your custom RAG action\n",
        "# The name \"rag_query_action\" here MUST match the name used in your Colang file.\n",
        "rails.register_action(action=rag_query_action, name=\"rag_query_action\")\n",
        "\n",
        "print(\"Nemo LLMRails initialized with Groq and custom RAG action ('rag_query_action') registered.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guHdWQYw7urG",
        "outputId": "d8df5872-d73d-4b0b-db66-be5c291a67a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected running in Colab/Jupyter. Applying nest_asyncio.\n",
            "\\nUser Query: What is Vizcaya Museum and Gardens?\n",
            "\n",
            "[Nemo Action LOG] Received query for RAG: What is Vizcaya Museum and Gardens?\n",
            "[Nemo Action LOG] Augmented prompt text being sent to LLM (first 200 chars): Using the contexts below, answer the query. If the context does not contain the answer, say that the provided context does not have the information.\n",
            "\n",
            "    Contexts:\n",
            "    Built in 1916 as a winter retrea...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:nemoguardrails.actions.action_dispatcher:Synchronous action `rag_query_action` has been called.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Nemo Action LOG] Cleaned LLM Content: The provided context does not have the information to answer the query about Vizcaya Museum and Gardens. Although the contexts describe various establishments with gardens, such as the Conservatory & Botanical Gardens and a Victorian-era greenhouse, none of them mention Vizcaya Museum and Gardens specifically. The first context describes a lavish villa built in 1916 as a winter retreat, which might be related to Vizcaya Museum and Gardens, but without a direct mention, it's impossible to confirm.\n",
            "[Nemo Action LOG] Concise Sources: \n",
            "\n",
            "**Here are some sources I used:**\n",
            "- Vizcaya Museum and Gardens (Source: https://www.tripadvisor.com/Attraction_Review-g34438-d130345-Reviews-Vizcaya_Museum_and_Gardens-Miami_Florida.html)\n",
            "- Bellagio Conservatory & Botanical Garden (Source: https://www.tripadvisor.com/Attraction_Review-g45963-d625114-Reviews-Bellagio_Conservatory_Botanical_Garden-Las_Vegas_Nevada.html)\n",
            "- Conservatory of Flowers (Source: https://www.tripadvisor.com/Attraction_Review-g60713-d117151-Reviews-Conservatory_of_Flowers-San_Francisco_California.html)\n",
            "[Nemo Action LOG] Final response being returned to Guardrails (first 300 chars): The provided context does not have the information to answer the query about Vizcaya Museum and Gardens. Although the contexts describe various establishments with gardens, such as the Conservatory & Botanical Gardens and a Victorian-era greenhouse, none of them mention Vizcaya Museum and Gardens sp...\n",
            "Bot Response: I'll try to find information on that using my knowledge base.\n",
            "I don't know the answer to that. However, I was able to find some information that might be helpful. Vizcaya Museum and Gardens appears to be a historic estate located in Miami, Florida, and it's possible that it was built as a winter retreat in 1916. For more accurate information, I recommend checking out the official website or reviews from sources like Tripadvisor. Would you like more information on how to plan a visit or learn more about its history?\n",
            "Is there anything else I can help you with regarding US travel or specific places?\n",
            "\\nUser Query: Is the President of the Thailand a good person?\n",
            "Bot Response: As a US travel assistant, I focus on providing information about places and travel. I don't discuss politics.\n",
            "Is there anything else I can help you with regarding US travel or specific places?\n",
            "\\nUser Query: Hello\n",
            "Bot Response: Hello! I'm your US travel assistant. How can I help you today?\n",
            "Is there anything else I can help you with regarding US travel or specific places?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1454307066.py:46: RuntimeWarning: coroutine 'main_test' was never awaited\n",
            "  asyncio.run(main_test())\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "async def get_response(query):\n",
        "    response = await rails.generate_async(messages=[{\"role\": \"user\", \"content\": query}])\n",
        "    return response.get(\"content\") # Or however you access the bot's message\n",
        "\n",
        "async def main_test():\n",
        "    # Test a query that should trigger the RAG action\n",
        "    query1 = \"What is Vizcaya Museum and Gardens?\"\n",
        "    print(f\"\\\\nUser Query: {query1}\")\n",
        "    bot_response1 = await get_response(query1)\n",
        "    print(f\"Bot Response: {bot_response1}\")\n",
        "\n",
        "    # Test a query that should trigger the politics guardrail\n",
        "    query2 = \"Is the President of the Thailand a good person?\"\n",
        "    print(f\"\\\\nUser Query: {query2}\")\n",
        "    bot_response2 = await get_response(query2)\n",
        "    print(f\"Bot Response: {bot_response2}\")\n",
        "\n",
        "    # Test a general greeting\n",
        "    query3 = \"Hello\"\n",
        "    print(f\"\\\\nUser Query: {query3}\")\n",
        "    bot_response3 = await get_response(query3)\n",
        "    print(f\"Bot Response: {bot_response3}\")\n",
        "\n",
        "# Run the async test\n",
        "# If you are in a Jupyter Notebook, you might need to run this with nest_asyncio or ensure an event loop is running.\n",
        "# For example:\n",
        "# import nest_asyncio\n",
        "# nest_asyncio.apply()\n",
        "# asyncio.run(main_test())\n",
        "\n",
        "# If running in a plain Python script:\n",
        "if __name__ == \"__main__\":\n",
        "    # To run asyncio code in a script or compatible environment\n",
        "    # For Jupyter, you might need:\n",
        "    # import nest_asyncio\n",
        "    # nest_asyncio.apply()\n",
        "    try:\n",
        "        asyncio.run(main_test())\n",
        "    except RuntimeError as e:\n",
        "        if \"asyncio.run() cannot be called from a running event loop\" in str(e) and 'google.colab' in str(type(get_ipython())):\n",
        "            print(\"Detected running in Colab/Jupyter. Applying nest_asyncio.\")\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "            asyncio.run(main_test())\n",
        "        else:\n",
        "            raise"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
