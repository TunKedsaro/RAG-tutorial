{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install google-genai"
      ],
      "metadata": {
        "id": "_YLmMNt0SdLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "GC-f4TFoSdIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AMCsy74SI7d"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyAUdcfHY_2IqEoPs_FuXizKBIxTqs0rpnU\"\n",
        "\n",
        "client = genai.Client(api_key=\"AIzaSyAUdcfHY_2IqEoPs_FuXizKBIxTqs0rpnU\")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model    = \"gemini-2.5-flash\",\n",
        "    contents = \"Hello, how are you?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "XwO0XmhBS769",
        "outputId": "4eebd453-2084-4d49-a277-3dbf7952e310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello! I'm doing well, thank you for asking.\\n\\nHow are you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>"
      ],
      "metadata": {
        "id": "fstpFUwMS9PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Mg5pzLwS98X",
        "outputId": "dda0dcad-4926-417a-94cf-4e5770a40b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber"
      ],
      "metadata": {
        "id": "cqs1swFiTOpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/05_resume.pdf\""
      ],
      "metadata": {
        "id": "-A4-fvZ5TnlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_text = \"\"\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    for page in pdf.pages:\n",
        "        resume_text = resume_text + page.extract_text()\n",
        "resume_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "ZvfFY7tLTRZO",
        "outputId": "9881bf24-4a36-4592-d45b-9128b3707cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Surya Teja Menta\\nData Scientist / ML Engineer / AI Engineer\\n+91 8309584461 mentasuryateja Surya-Teja-Menta surya-teja-menta suryatejamenta.co.in\\nAddress: 3-12-16, Kotha Bazar, Kavali (524201), Andhra Pradesh, India\\nSummary\\nI’m Surya Teja Menta, Results-driven Senior Data Scientist with 4+ years of experience in Data Science,\\nMachine Learning (ML), and Generative AI (GenAI). Proven expertise in RAG (Retrieval-Augmented\\nGeneration), LLM fine-tuning, MLOps, and end-to-end AI solutions. Strong background in data analytics, statistical\\nmodeling, AI-powered automation, and scalable AI architectures. IBM Certified Professional Data Scientist with\\nhands-on experience in LangChain, Hugging Face, OpenAI APIs, Vector Databases (ChromaDB, Pinecone), and\\ncloud deployments (AWS, GCP). Passionate about AI research, model optimization, and developing cutting-edge AI\\nsolutions.\\nWork Experience\\n- Senior Data Scientist - Robert Bosch (Jan 2024 - preset)\\nDesigned and deployed custom YOLOv8 models for computer vision applications. Built multi-modal RAG\\npipelines for intelligent document processing, contextual retrieval, and GenAI-powered insights. Worked on\\nTime series data for Adnoc Project. Developed ML algorithms for signal processing and sensor vibration data\\nanalytics using scikit-learn, PyTorch, and TensorFlow. Worked on LLM fine-tuning, prompt engineering, and\\nefficient model inference for enterprise AI applications. Integrated MLOps best practices for scalable AI pipelines,\\nmodel versioning, and automated deployment.\\n- Subject Matter Expert (Data Analyst) - Tudip Technologies (Oct 2020 - Oct 2023)\\nDeveloped interactive analytics dashboards and data reports in Google Data Studio for content management\\nanalytics. Automated text summarization & paraphrasing using NLP Transformers (BART, T5) and deployed on\\nGoogle Cloud Run. Built AI-powered data visualization tools to enhance real-time business intelligence and trend\\nanalysis. Integrated LLM-powered chatbots with Google Sheets API to improve data accessibility across teams.\\nRecognized with the Best Team Award for highest client appreciation and impactful AI-driven automation.\\nProjects\\n- Advanced RAG Research Assistant (LangChain, LLM, Hugging Face, Vector DB, AI, NLP)\\nBuilt an AI-powered Retrieval-Augmented Generation (RAG) system for intelligent document analysis\\nwith multi-modal PDF processing, semantic chunking, and adaptive query handling. Integrated\\nvector-based semantic search (ChromaDB), BM25 retrieval, and contextual response generation using\\nLlama3.2 and Hugging Face. Developed context-tracking algorithms for personalized, stateful interactions.\\nTech stack: Python, LangChain, Chainlit, OpenAI API.\\n- Github Automated Analysis (OpenAI, Langchain, LLM)\\nAutomated Complexity Analysis: Engineered a Python tool using GPT and LangChain to pinpoint intricate GitHub\\nrepositories. Memory Optimization: Implemented efficient memory management for processing extensive files while\\nadhering to token limits. Enhanced Complexity Scoring: Utilized prompt engineering to elevate accuracy in\\nevaluating repository complexity, refining the precision of the tool.\\n- Re-Enhance.AI (Opencv, ESRGAN, Pytorch)\\nThe Re-Enhance.AI project is a set of tools and algorithms that can be used to improve the quality of your\\nImage for Space & Research Purposes. Even though the model accepts only images with (256x256x3) dimensions,\\nthis framework manages the higher dimensions by Split and Send Policy.Professional skills\\nKnowledge: Data Science, Machine Learning, Deep, NLP & CV, LLMs, Fine Tuning LLMs, Gen AI,OpenCV, YOLOv8,\\nImage Processing, Object Detection, GANs, Signal Processing & Time Series Analysis\\nLanguages: Python, SQL, HTML, CSS, Web development,SQL, NoSQL\\nFrameworks: Tensorflow, Pytorch, Keras, Langchain\\nCloud: GCP Cloud Run, Amazon Sage Maker(AWS) basic\\nLibraries: Numpy, Pandas, Sklearn, Spacy, NLTK, Opencv, Matplotlib / Seaborn, scipy\\nTools: Jupyter notebooks, docker, GitHub, Git, Google DataStudio\\nOthers: Statistics & Probability, Calculus, Mathematics, Linear Algebra, Worked on Complex Datasets, Statistical\\nAnalysis, Data Mining, Model Building and Deploying, Hypothesis Testing, Data Analytics.\\nEducation\\nBachelor's Degree(CS) - PBR VITS, Kavali, AP (June 2016 - May 2020)\\nI have done my Bachelor's degree in Computer Science with 78.3%. I have participated in paper\\npresentations and won prizes too.\\nHSC - Narayana Junior College, Kavali, AP (June 2014 - May 2016)\\nI have completed my HSC education with a 92.5%\\nSSC - Kranthi EM School, Kavali, AP (June 2004 - May 2014)\\nI have done my schooling with 87%. I also participated in school dramas, sports, etc.\\nCertificates & Courses\\n● IBM Professional Data Scientist Certificate\\n● Advanced Deep Learning course in Ineuron.AI\\n● Python & Web Development\\nSoft Skills\\nAnalytical Thinking: Demonstrated through experience in data analysis, statistical techniques, and training on\\ncomplex datasets.\\nProblem Solving: Interpret data, Implementing algorithms, and finding solutions. used problem-solving skills to\\nsolve challenges.\\nCollaboration: Highlighted by working with Google on the Google LX Project, collaborating with clients, and creating\\nanalytics dashboards and reports to meet their requirements.\\nCommunication: Evident through writing AI/ML blogs, presenting papers, and effectively communicating complex\\nconcepts to both technical and non-technical stakeholders.\\nContinuous Learning: Exhibited by pursuing advanced courses and certifications in data science, machine learning,\\nand deep learning.\\nAdaptability: learning new technologies, and actively contributing to AI/ML communities. I'm tech savvy.\\nTime Management: Proven ability to handle multiple projects simultaneously while meeting deadlines\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(resume_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsIxwYHyTwXY",
        "outputId": "ef49a715-4674-4be2-ae6b-81bfa83a7a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Surya Teja Menta\n",
            "Data Scientist / ML Engineer / AI Engineer\n",
            "+91 8309584461 mentasuryateja Surya-Teja-Menta surya-teja-menta suryatejamenta.co.in\n",
            "Address: 3-12-16, Kotha Bazar, Kavali (524201), Andhra Pradesh, India\n",
            "Summary\n",
            "I’m Surya Teja Menta, Results-driven Senior Data Scientist with 4+ years of experience in Data Science,\n",
            "Machine Learning (ML), and Generative AI (GenAI). Proven expertise in RAG (Retrieval-Augmented\n",
            "Generation), LLM fine-tuning, MLOps, and end-to-end AI solutions. Strong background in data analytics, statistical\n",
            "modeling, AI-powered automation, and scalable AI architectures. IBM Certified Professional Data Scientist with\n",
            "hands-on experience in LangChain, Hugging Face, OpenAI APIs, Vector Databases (ChromaDB, Pinecone), and\n",
            "cloud deployments (AWS, GCP). Passionate about AI research, model optimization, and developing cutting-edge AI\n",
            "solutions.\n",
            "Work Experience\n",
            "- Senior Data Scientist - Robert Bosch (Jan 2024 - preset)\n",
            "Designed and deployed custom YOLOv8 models for computer vision applications. Built multi-modal RAG\n",
            "pipelines for intelligent document processing, contextual retrieval, and GenAI-powered insights. Worked on\n",
            "Time series data for Adnoc Project. Developed ML algorithms for signal processing and sensor vibration data\n",
            "analytics using scikit-learn, PyTorch, and TensorFlow. Worked on LLM fine-tuning, prompt engineering, and\n",
            "efficient model inference for enterprise AI applications. Integrated MLOps best practices for scalable AI pipelines,\n",
            "model versioning, and automated deployment.\n",
            "- Subject Matter Expert (Data Analyst) - Tudip Technologies (Oct 2020 - Oct 2023)\n",
            "Developed interactive analytics dashboards and data reports in Google Data Studio for content management\n",
            "analytics. Automated text summarization & paraphrasing using NLP Transformers (BART, T5) and deployed on\n",
            "Google Cloud Run. Built AI-powered data visualization tools to enhance real-time business intelligence and trend\n",
            "analysis. Integrated LLM-powered chatbots with Google Sheets API to improve data accessibility across teams.\n",
            "Recognized with the Best Team Award for highest client appreciation and impactful AI-driven automation.\n",
            "Projects\n",
            "- Advanced RAG Research Assistant (LangChain, LLM, Hugging Face, Vector DB, AI, NLP)\n",
            "Built an AI-powered Retrieval-Augmented Generation (RAG) system for intelligent document analysis\n",
            "with multi-modal PDF processing, semantic chunking, and adaptive query handling. Integrated\n",
            "vector-based semantic search (ChromaDB), BM25 retrieval, and contextual response generation using\n",
            "Llama3.2 and Hugging Face. Developed context-tracking algorithms for personalized, stateful interactions.\n",
            "Tech stack: Python, LangChain, Chainlit, OpenAI API.\n",
            "- Github Automated Analysis (OpenAI, Langchain, LLM)\n",
            "Automated Complexity Analysis: Engineered a Python tool using GPT and LangChain to pinpoint intricate GitHub\n",
            "repositories. Memory Optimization: Implemented efficient memory management for processing extensive files while\n",
            "adhering to token limits. Enhanced Complexity Scoring: Utilized prompt engineering to elevate accuracy in\n",
            "evaluating repository complexity, refining the precision of the tool.\n",
            "- Re-Enhance.AI (Opencv, ESRGAN, Pytorch)\n",
            "The Re-Enhance.AI project is a set of tools and algorithms that can be used to improve the quality of your\n",
            "Image for Space & Research Purposes. Even though the model accepts only images with (256x256x3) dimensions,\n",
            "this framework manages the higher dimensions by Split and Send Policy.Professional skills\n",
            "Knowledge: Data Science, Machine Learning, Deep, NLP & CV, LLMs, Fine Tuning LLMs, Gen AI,OpenCV, YOLOv8,\n",
            "Image Processing, Object Detection, GANs, Signal Processing & Time Series Analysis\n",
            "Languages: Python, SQL, HTML, CSS, Web development,SQL, NoSQL\n",
            "Frameworks: Tensorflow, Pytorch, Keras, Langchain\n",
            "Cloud: GCP Cloud Run, Amazon Sage Maker(AWS) basic\n",
            "Libraries: Numpy, Pandas, Sklearn, Spacy, NLTK, Opencv, Matplotlib / Seaborn, scipy\n",
            "Tools: Jupyter notebooks, docker, GitHub, Git, Google DataStudio\n",
            "Others: Statistics & Probability, Calculus, Mathematics, Linear Algebra, Worked on Complex Datasets, Statistical\n",
            "Analysis, Data Mining, Model Building and Deploying, Hypothesis Testing, Data Analytics.\n",
            "Education\n",
            "Bachelor's Degree(CS) - PBR VITS, Kavali, AP (June 2016 - May 2020)\n",
            "I have done my Bachelor's degree in Computer Science with 78.3%. I have participated in paper\n",
            "presentations and won prizes too.\n",
            "HSC - Narayana Junior College, Kavali, AP (June 2014 - May 2016)\n",
            "I have completed my HSC education with a 92.5%\n",
            "SSC - Kranthi EM School, Kavali, AP (June 2004 - May 2014)\n",
            "I have done my schooling with 87%. I also participated in school dramas, sports, etc.\n",
            "Certificates & Courses\n",
            "● IBM Professional Data Scientist Certificate\n",
            "● Advanced Deep Learning course in Ineuron.AI\n",
            "● Python & Web Development\n",
            "Soft Skills\n",
            "Analytical Thinking: Demonstrated through experience in data analysis, statistical techniques, and training on\n",
            "complex datasets.\n",
            "Problem Solving: Interpret data, Implementing algorithms, and finding solutions. used problem-solving skills to\n",
            "solve challenges.\n",
            "Collaboration: Highlighted by working with Google on the Google LX Project, collaborating with clients, and creating\n",
            "analytics dashboards and reports to meet their requirements.\n",
            "Communication: Evident through writing AI/ML blogs, presenting papers, and effectively communicating complex\n",
            "concepts to both technical and non-technical stakeholders.\n",
            "Continuous Learning: Exhibited by pursuing advanced courses and certifications in data science, machine learning,\n",
            "and deep learning.\n",
            "Adaptability: learning new technologies, and actively contributing to AI/ML communities. I'm tech savvy.\n",
            "Time Management: Proven ability to handle multiple projects simultaneously while meeting deadlines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>"
      ],
      "metadata": {
        "id": "Q4TxjtfRT4G6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1 Role_Relevance.Skill_match"
      ],
      "metadata": {
        "id": "0bd-OVvUXHk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are an expert resume parser.\n",
        "\n",
        "From the following resume text, extract only the candidate's SKILLS as a clean Python list of strings.\n",
        "Include both technical (e.g., Python, SQL, Machine Learning) and non-technical (e.g., Communication, Leadership) skills if found.\n",
        "\n",
        "Return only valid JSON in this exact format:\n",
        "{{\n",
        "    \"skills\":[\"skill1\",\"skill2\",\"skill3\",...]\n",
        "}}\n",
        "\n",
        "Resume:\n",
        "{resume_text}\n",
        "\"\"\"\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt\n",
        ")"
      ],
      "metadata": {
        "id": "TUCGD1gmW9Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Rskills = json.loads(response.text.split('```json')[1].split('```')[0])\n",
        "Rskills = Rskills['skills']"
      ],
      "metadata": {
        "id": "wtUgj35_X3aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Rskills"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWnnVYc6YMPQ",
        "outputId": "62116d96-eb40-497f-b6e4-ef7133191ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Adaptability',\n",
              " 'Adaptive query handling',\n",
              " 'AI-driven automation',\n",
              " 'AI-powered automation',\n",
              " 'AI-powered data visualization',\n",
              " 'AI research',\n",
              " 'AI solutions',\n",
              " 'Amazon SageMaker',\n",
              " 'Analytical Thinking',\n",
              " 'Automated deployment',\n",
              " 'AWS',\n",
              " 'BART',\n",
              " 'BM25 retrieval',\n",
              " 'Calculus',\n",
              " 'Chainlit',\n",
              " 'ChromaDB',\n",
              " 'Collaboration',\n",
              " 'Communication',\n",
              " 'Computer Vision',\n",
              " 'Content management analytics',\n",
              " 'Continuous Learning',\n",
              " 'Contextual retrieval',\n",
              " 'Contextual response generation',\n",
              " 'CSS',\n",
              " 'CV',\n",
              " 'Data analytics',\n",
              " 'Data Mining',\n",
              " 'Data reports',\n",
              " 'Data Science',\n",
              " 'Deep Learning',\n",
              " 'Docker',\n",
              " 'ESRGAN',\n",
              " 'GANs',\n",
              " 'GCP',\n",
              " 'GenAI',\n",
              " 'Generative AI',\n",
              " 'GenAI-powered insights',\n",
              " 'Git',\n",
              " 'GitHub',\n",
              " 'Google Cloud Run',\n",
              " 'Google Data Studio',\n",
              " 'Google Sheets API',\n",
              " 'GPT',\n",
              " 'HTML',\n",
              " 'Hugging Face',\n",
              " 'Hypothesis Testing',\n",
              " 'Image Processing',\n",
              " 'Intelligent document processing',\n",
              " 'Interactive analytics dashboards',\n",
              " 'Jupyter notebooks',\n",
              " 'Keras',\n",
              " 'LangChain',\n",
              " 'Linear Algebra',\n",
              " 'LLM fine-tuning',\n",
              " 'LLM-powered chatbots',\n",
              " 'LLMs',\n",
              " 'Llama3.2',\n",
              " 'Machine Learning',\n",
              " 'Mathematics',\n",
              " 'Matplotlib',\n",
              " 'Memory Optimization',\n",
              " 'ML',\n",
              " 'ML algorithms',\n",
              " 'MLOps',\n",
              " 'MLOps best practices',\n",
              " 'Model Building',\n",
              " 'Model Deploying',\n",
              " 'Model inference',\n",
              " 'Model optimization',\n",
              " 'Model versioning',\n",
              " 'Multi-modal PDF processing',\n",
              " 'Multi-modal RAG pipelines',\n",
              " 'Natural Language Processing (NLP)',\n",
              " 'NLTK',\n",
              " 'NoSQL',\n",
              " 'Numpy',\n",
              " 'Object Detection',\n",
              " 'OpenAI APIs',\n",
              " 'OpenCV',\n",
              " 'Pandas',\n",
              " 'Paraphrasing',\n",
              " 'Pinecone',\n",
              " 'Problem Solving',\n",
              " 'Prompt engineering',\n",
              " 'Python',\n",
              " 'PyTorch',\n",
              " 'RAG',\n",
              " 'Real-time business intelligence',\n",
              " 'Retrieval-Augmented Generation',\n",
              " 'Scalable AI architectures',\n",
              " 'scikit-learn',\n",
              " 'SciPy',\n",
              " 'Seaborn',\n",
              " 'Semantic chunking',\n",
              " 'Sensor vibration data analytics',\n",
              " 'Signal processing',\n",
              " 'SQL',\n",
              " 'Spacy',\n",
              " 'Statistical Analysis',\n",
              " 'Statistical modeling',\n",
              " 'Statistics & Probability',\n",
              " 'T5',\n",
              " 'TensorFlow',\n",
              " 'Text summarization',\n",
              " 'Time Management',\n",
              " 'Time series data',\n",
              " 'Trend analysis',\n",
              " 'Vector Databases',\n",
              " 'Vector-based semantic search',\n",
              " 'Web development',\n",
              " 'Working with Complex Datasets',\n",
              " 'YOLOv8']"
            ]
          },
          "metadata": {},
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Iskills = [\"Python\", \"R\", \"SQL\", \"Scala\", \"Julia\", \"Java\",\n",
        "\"Linear Algebra\", \"Calculus\", \"Probability\", \"Descriptive Statistics\", \"Inferential Statistics\", \"Hypothesis Testing\", \"Bayesian Statistics\",\n",
        "\"Supervised Learning\", \"Unsupervised Learning\", \"Reinforcement Learning\", \"Neural Networks\", \"Convolutional Neural Networks (CNN)\", \"Recurrent Neural Networks (RNN)\", \"Transformers\", \"Gradient Boosting\", \"Decision Trees\", \"Ensemble Learning\", \"Feature Engineering\", \"Model Evaluation\", \"Hyperparameter Tuning\",\n",
        "\"Pandas\", \"NumPy\", \"Scikit-learn\", \"SciPy\", \"Statsmodels\", \"Dask\", \"Polars\",\n",
        "\"Matplotlib\", \"Seaborn\", \"Plotly\", \"Power BI\", \"Tableau\", \"ggplot2\", \"Altair\", \"Dash\",\n",
        "\"MySQL\", \"PostgreSQL\", \"MongoDB\", \"Hadoop\", \"Spark\", \"Hive\", \"Snowflake\", \"BigQuery\", \"Redshift\",\n",
        "\"AWS\", \"GCP\", \"Azure\", \"Docker\", \"Kubernetes\", \"CI/CD\", \"MLflow\", \"DVC\", \"Kubeflow\", \"SageMaker\", \"Vertex AI\",\n",
        "\"ETL\", \"Data Warehousing\", \"Airflow\", \"Kafka\", \"Data Pipeline Design\",\n",
        "\"Text Preprocessing\", \"Tokenization\", \"Word Embeddings\", \"Transformers\", \"BERT\", \"LLMs\", \"Sentiment Analysis\", \"Topic Modeling\",\n",
        "\"Communication\", \"Problem Solving\", \"Critical Thinking\", \"Storytelling with Data\", \"Business Acumen\", \"Collaboration\",\n",
        "\"Git\", \"GitHub\", \"Jupyter Notebook\", \"VS Code\", \"PyCharm\", \"Linux\", \"Bash\",\n",
        "\"Finance\", \"Healthcare\", \"Marketing Analytics\", \"Supply Chain\", \"Geospatial Analysis\", \"Computer Vision\"]"
      ],
      "metadata": {
        "id": "vkEgC7IDYPoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Rsk = set(Rskills)\n",
        "Isk = set(Iskills)"
      ],
      "metadata": {
        "id": "Aa4zMU_9YRPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jaccard_score = len(Rsk&Isk)/len(Rsk|Isk)\n",
        "# jaccard_score"
      ],
      "metadata": {
        "id": "ODVHVbsrYS09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = len(Rsk&Isk)\n",
        "print(f\"score : {score/len(Isk):.5f} ({score}/{len(Isk)})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDpobi77YUEZ",
        "outputId": "69d0f1cd-14b4-4c3c-dd3d-e34c88fb3932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score : 0.20652 (19/92)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "point_skill_match = score/len(Isk)*50\n",
        "point_skill_match"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YiyjaK9YjTn",
        "outputId": "7323a1db-4cbf-4ffe-f2d3-7c65569f2fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.326086956521738"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Role_relevance.Experience_relevance"
      ],
      "metadata": {
        "id": "SRrrh1R0YvLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JD\n",
        "# source : https://th.jobsdb.com/th/job/88415472?type=standard&ref=search-standalone&origin=cardTitle#sol=edc792a5ac547c3da985c96f6aa73d80fa3df3d7\n",
        "jd_text = '''\n",
        "Data Scientist (Mid-Level)\n",
        "\n",
        "Bank of Ayudhya Public Company Limited (Ayudhya Capital Auto)\n",
        "ดูงานทั้งหมด\n",
        "ยานนาวา กรุงเทพมหานคร (ไฮบริด)\n",
        "งานคณิตศาสตร์ งานสถิติ งานสารสนเทศศาสตร์ (งานวิทยาศาสตร์ งานเทคโนโลยี)\n",
        "งานเต็มเวลา\n",
        "เพิ่มเงินเดือนที่คาดหวังเข้าไปในโปรไฟล์ของคุณสำหรับข้อมูลเชิงลึก\n",
        "Posted 1 วันที่ผ่านมา\n",
        "\n",
        "Responsibilities\n",
        "Identify valuable data sources and automate data collection processes.\n",
        "\n",
        "Undertake preprocessing and transformation of structured and unstructured data.\n",
        "\n",
        "Analyze large datasets to discover meaningful trends, patterns, and business insights.\n",
        "\n",
        "Build and deploy predictive models, machine learning algorithms, and NLP models (classification, clustering, topic modeling, sentiment analysis).\n",
        "\n",
        "Combine models through ensemble techniques to improve accuracy.\n",
        "\n",
        "Design and maintain end-to-end data pipelines and migrate workflows to cloud platforms (e.g., Databricks, BigQuery, PySpark).\n",
        "\n",
        "Develop and maintain dashboards (Power BI, Looker, Tableau) to communicate insights effectively.\n",
        "\n",
        "Collaborate with cross-functional teams including product, engineering, and business stakeholders to align data solutions with business needs.\n",
        "\n",
        "Propose solutions and strategies to address complex business challenges.\n",
        "\n",
        "Qualifications\n",
        "Bachelor’s or Master's degree in Data Science, Statistics, or related field\n",
        "\n",
        "Proven experience as a Data Scientist or Data Analyst, with demonstrated success in building and deploying data-driven solutions.\n",
        "\n",
        "Strong proficiency in Python, SQL, PySpark; experience with big data platforms (Databricks, BigQuery, Redshift, Hadoop).\n",
        "\n",
        "Solid understanding of machine learning and deep learning frameworks.\n",
        "\n",
        "Experience in data mining, feature engineering, and statistical modeling.\n",
        "\n",
        "Strong math and statistical skills; analytical mindset with problem-solving aptitude.\n",
        "\n",
        "Excellent communication and presentation skills to engage with both technical and non-technical stakeholders.\n",
        "\n",
        "(Preferred) Experience with stream processing, NLP, and scalable ‘big data’ stores.\n",
        "\n",
        "Applicants can read the Personal Data Protection Announcement of the Bank's Human Resources Function by typing the link from the image that stated below\n",
        "\n",
        "EN: (https://krungsri.com/b/privacynoticeen)\n",
        "\n",
        "ผู้สมัครสามารถอ่านประกาศการคุ้มครองข้อมูลส่วนบุคคลส่วนงานทรัพยากรบุคคลของธนาคารได้โดยการพิมพ์ลิงค์จากรูปภาพที่ปรากฎด้านล่าง\n",
        "\n",
        "ภาษาไทย: (https://krungsri.com/b/privacynoticeth)\n",
        "\n",
        "หมายเหตุ ธนาคารมีความจำเป็นและจะมีขั้นตอนการตรวจสอบข้อมูลส่วนบุคคลเกี่ยวกับประวัติอาชญากรรมของผู้สมัคร ก่อนที่ผู้สมัครจะได้รับการพิจารณาเข้าร่วมงานกับธนาคารกรุงศรีฯ\n",
        "\n",
        "Remark: The bank needs to and will have a process for verifying personal information related to the criminal history of applicants before they are considered for employment with the bank.\n",
        "\n",
        "\n",
        "\n",
        "ปลดล็อกข้อมูลเชิงลึกของตำแหน่งงาน\n",
        "เงินเดือนตรงตามต้องการ\n",
        "จำนวนผู้สมัคร\n",
        "ทักษะตรงกับที่คุณมี\n",
        "คำถามจากผู้ประกอบการ\n",
        "ใบสมัครของคุณจะประกอบด้วยคำถามต่อไปนี้:\n",
        "Which of the following languages are you fluent in?\n",
        "Which of the following statements best describes your right to work in Thailand?\n",
        "Which of the following programming languages are you experienced in?\n",
        "What's your expected monthly basic salary?\n",
        "Have you worked in a role which requires experience with machine learning techniques?\n",
        "Which of the following types of qualifications do you have?\n",
        "How many years' experience do you have using SQL queries?\n",
        "How many years' experience do you have as a Data Scientist?\n",
        "'''"
      ],
      "metadata": {
        "id": "wvzL1jutYrzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are an HR evaluation assistant specialized in data science recruitment.\n",
        "\n",
        "You will be given:\n",
        "1. A job description (JD)\n",
        "2. A candidate's extracted resume data in JSON format\n",
        "\n",
        "Your task:\n",
        "- Compare the candidate’s experience, skills, and qualifications against the JD requirements.\n",
        "- Focus on *experience relevance*, not education or soft skills.\n",
        "- Evaluate how closely the candidate's actual experience and technical skills match the job's responsibilities and required tools.\n",
        "- Consider alignment in tools (Python, SQL, PySpark, Power BI, etc.), ML techniques, data pipeline experience, and communication requirements.\n",
        "\n",
        "Output format (JSON only):\n",
        "\n",
        "{{\n",
        "  \"experience_relevance_score\": <0-50>,\n",
        "  \"explanation\": [\n",
        "    \"Matched on machine learning and NLP experience.\",\n",
        "    \"Has Python and SQL but no PySpark or cloud platform experience.\",\n",
        "    \"Good visualization tools but limited big data exposure.\"\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Scoring guide:\n",
        "- 0–10: Very low relevance (different domain or junior)\n",
        "- 11–25: Partial match (some skills overlap)\n",
        "- 26–40: Good fit (most tools and responsibilities match)\n",
        "- 41–50: Excellent fit (direct match to required experience)\n",
        "\n",
        "### JOB DESCRIPTION ###\n",
        "{jd_text}\n",
        "\n",
        "### CANDIDATE RESUME ###\n",
        "{resume_json}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "pvS7fYOhZ6UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ6jaOcaadqk",
        "outputId": "a8e7d0a4-117d-4319-8589-4b5bc3ce596e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are an HR evaluation assistant specialized in data science recruitment.\n",
            "\n",
            "You will be given:\n",
            "1. A job description (JD)\n",
            "2. A candidate's extracted resume data in JSON format\n",
            "\n",
            "Your task:\n",
            "- Compare the candidate’s experience, skills, and qualifications against the JD requirements.\n",
            "- Focus on *experience relevance*, not education or soft skills.\n",
            "- Evaluate how closely the candidate's actual experience and technical skills match the job's responsibilities and required tools.\n",
            "- Consider alignment in tools (Python, SQL, PySpark, Power BI, etc.), ML techniques, data pipeline experience, and communication requirements.\n",
            "\n",
            "Output format (JSON only):\n",
            "\n",
            "{\n",
            "  \"experience_relevance_score\": <0-50>,\n",
            "  \"explanation\": [\n",
            "    \"Matched on machine learning and NLP experience.\",\n",
            "    \"Has Python and SQL but no PySpark or cloud platform experience.\",\n",
            "    \"Good visualization tools but limited big data exposure.\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "Scoring guide:\n",
            "- 0–10: Very low relevance (different domain or junior)\n",
            "- 11–25: Partial match (some skills overlap)\n",
            "- 26–40: Good fit (most tools and responsibilities match)\n",
            "- 41–50: Excellent fit (direct match to required experience)\n",
            "\n",
            "### JOB DESCRIPTION ###\n",
            "\n",
            "Data Scientist (Mid-Level)\n",
            "\n",
            "Bank of Ayudhya Public Company Limited (Ayudhya Capital Auto)\n",
            "ดูงานทั้งหมด\n",
            "ยานนาวา กรุงเทพมหานคร (ไฮบริด)\n",
            "งานคณิตศาสตร์ งานสถิติ งานสารสนเทศศาสตร์ (งานวิทยาศาสตร์ งานเทคโนโลยี)\n",
            "งานเต็มเวลา\n",
            "เพิ่มเงินเดือนที่คาดหวังเข้าไปในโปรไฟล์ของคุณสำหรับข้อมูลเชิงลึก\n",
            "Posted 1 วันที่ผ่านมา\n",
            "\n",
            "Responsibilities\n",
            "Identify valuable data sources and automate data collection processes.\n",
            "\n",
            "Undertake preprocessing and transformation of structured and unstructured data.\n",
            "\n",
            "Analyze large datasets to discover meaningful trends, patterns, and business insights.\n",
            "\n",
            "Build and deploy predictive models, machine learning algorithms, and NLP models (classification, clustering, topic modeling, sentiment analysis).\n",
            "\n",
            "Combine models through ensemble techniques to improve accuracy.\n",
            "\n",
            "Design and maintain end-to-end data pipelines and migrate workflows to cloud platforms (e.g., Databricks, BigQuery, PySpark).\n",
            "\n",
            "Develop and maintain dashboards (Power BI, Looker, Tableau) to communicate insights effectively.\n",
            "\n",
            "Collaborate with cross-functional teams including product, engineering, and business stakeholders to align data solutions with business needs.\n",
            "\n",
            "Propose solutions and strategies to address complex business challenges.\n",
            "\n",
            "Qualifications\n",
            "Bachelor’s or Master's degree in Data Science, Statistics, or related field\n",
            "\n",
            "Proven experience as a Data Scientist or Data Analyst, with demonstrated success in building and deploying data-driven solutions.\n",
            "\n",
            "Strong proficiency in Python, SQL, PySpark; experience with big data platforms (Databricks, BigQuery, Redshift, Hadoop).\n",
            "\n",
            "Solid understanding of machine learning and deep learning frameworks.\n",
            "\n",
            "Experience in data mining, feature engineering, and statistical modeling.\n",
            "\n",
            "Strong math and statistical skills; analytical mindset with problem-solving aptitude.\n",
            "\n",
            "Excellent communication and presentation skills to engage with both technical and non-technical stakeholders.\n",
            "\n",
            "(Preferred) Experience with stream processing, NLP, and scalable ‘big data’ stores.\n",
            "\n",
            "Applicants can read the Personal Data Protection Announcement of the Bank's Human Resources Function by typing the link from the image that stated below\n",
            "\n",
            "EN: (https://krungsri.com/b/privacynoticeen)\n",
            "\n",
            "ผู้สมัครสามารถอ่านประกาศการคุ้มครองข้อมูลส่วนบุคคลส่วนงานทรัพยากรบุคคลของธนาคารได้โดยการพิมพ์ลิงค์จากรูปภาพที่ปรากฎด้านล่าง\n",
            "\n",
            "ภาษาไทย: (https://krungsri.com/b/privacynoticeth)\n",
            "\n",
            "หมายเหตุ ธนาคารมีความจำเป็นและจะมีขั้นตอนการตรวจสอบข้อมูลส่วนบุคคลเกี่ยวกับประวัติอาชญากรรมของผู้สมัคร ก่อนที่ผู้สมัครจะได้รับการพิจารณาเข้าร่วมงานกับธนาคารกรุงศรีฯ\n",
            "\n",
            "Remark: The bank needs to and will have a process for verifying personal information related to the criminal history of applicants before they are considered for employment with the bank.\n",
            "\n",
            "\n",
            "\n",
            "ปลดล็อกข้อมูลเชิงลึกของตำแหน่งงาน\n",
            "เงินเดือนตรงตามต้องการ\n",
            "จำนวนผู้สมัคร\n",
            "ทักษะตรงกับที่คุณมี\n",
            "คำถามจากผู้ประกอบการ\n",
            "ใบสมัครของคุณจะประกอบด้วยคำถามต่อไปนี้:\n",
            "Which of the following languages are you fluent in?\n",
            "Which of the following statements best describes your right to work in Thailand?\n",
            "Which of the following programming languages are you experienced in?\n",
            "What's your expected monthly basic salary?\n",
            "Have you worked in a role which requires experience with machine learning techniques?\n",
            "Which of the following types of qualifications do you have?\n",
            "How many years' experience do you have using SQL queries?\n",
            "How many years' experience do you have as a Data Scientist?\n",
            "\n",
            "\n",
            "### CANDIDATE RESUME ###\n",
            "{'contact_information': {'name': 'Yuchen (Winnie) Shao', 'email': '-', 'phone': '-', 'linkedin': 'https://www.linkedin.com/in/yuchenshao/', 'jobdb_link': '-', 'portfolio_link': '-'}, 'professional_summary': {'has_summary': 'No', 'summary_points': []}, 'education': [{'institution': 'Northwestern University', 'degree': 'MS, Analytics', 'dates': 'Sep 2021 - Expected Dec 2022', 'gpa': '-', 'honors': '-'}, {'institution': 'University of Washington', 'degree': 'BS, Data Science & Statistics(Applied ComputationalMathematics Science)', 'dates': 'Sep 2017 – June 2021', 'gpa': '3.8/4.0', 'honors': '-'}], 'experience': [{'title': 'Data Science Intern', 'company': 'Digital Ocean', 'location': 'Seattle, WA', 'dates': 'June 2021 - Aug 2021', 'description': ['Established an NLP classification model to sort customer churn responses into 8 categories with Python', 'Utilized SQL to retrieve customer churn responses, built a program to auto-translate non-English response, and generated word clouds and sentiment distribution on cleaned dataset', 'Constructed an SVM Classifier with stochastic gradient descent optimization method on labeled dataset, implemented 10-fold cross validation to find the best set of hyperparameters for the model and increased the accuracy by 8 percent', 'Demonstrated the relationship between monthly recognized revenue and different types of churn, and provided insights on future strategy towards churned customers', 'Presented the final results to the business operation team to help them understand why customers churned.']}, {'title': 'Data Analyst Intern', 'company': 'Suzhou Academy of Planning & Design', 'location': 'China', 'dates': 'June 2019 – Aug 2019', 'description': ['Attained a guidance of discount strategy to attract more people applying yearly commute cards by visualizing data flow of public transportation population with Python, Matplotlib, and Tableau', 'Executed data cleaning on 6 million public transportation data of 2018 in Suzhou by consolidating values and removing outliers', 'Evaluated and delivered project progress and strategy insights to executive management for transportation discount policy optimization', 'Utilized MySQL to retrieve card information from transportation table and manipulated data through Python and R']}, {'title': 'Amazon Food Review (NLP, Python)', 'company': 'Personal Project', 'location': '-', 'dates': 'March 2021', 'description': ['Formed a classification model with logistics regression through data cleaning and feature engineering as well as visualized the importance of each feature in the model through seaborn and matplotlib', 'Evaluated the model with ROC, AUC, and confusion matrix and warehoused the model using pickle package']}, {'title': 'Conversion Rate Analysis (Python, Jupyter notebook)', 'company': 'Personal Project', 'location': '-', 'dates': 'March 2021', 'description': ['Determined general user profiles and association between conversion and user profile with exploratory data analysis and discriminated converted and unconverted people through data visualization', 'Applied a random forest classification model to predict the conversion rate of the e-commerce website.', 'Interpreted the model and feature importance, provided general guidance of target audience and ideas to improve conversion rate']}, {'title': 'Predicting Spam Messages from SMS Message Board(NLP, Python)', 'company': 'Personal Project', 'location': '-', 'dates': 'December 2020', 'description': ['Conducted exploratory data analysis by using Numpy, Matplotlib, Pandas, and WordCloud package in Python and employed NLTK package to tokenize messages and extract features including word count, URL count, special punctuation count and TF-IDF', 'Performed forecasting using classifier model such as Decision Tree Classifier, Naïve Bayes Classifier, and Random Forest Classifier', 'Achieved 0.98 accuracy through directing 5-fold cross validation on all Classifier models through SVM Classifier']}, {'title': 'King County House Price Analysis(R)', 'company': 'Personal Project', 'location': '-', 'dates': 'December 2020', 'description': ['Recognized relation among 15 key features by carrying out data analysis on housing price data', 'Forecasted house price by visualization on King County map to stain the house price distribution and potential key features', 'Performed 5-fold cross validation on 14 different best subset model, and identified cross validation error for multiple subsets', 'Enhanced the RMSE by 0.05 using 5 key features subset while using all the variables as predictors provided RMSE 0.27']}], 'skills': {'technical': ['Python', 'Pandas', 'Numpy', 'NLTK', 'R', 'Java', 'SQL', 'Decision Tree', 'Random Forest', 'SVM', 'Feature Engineering', 'Logistics Regression', 'Exploratory Data Analysis', 'Survival Analysis', 'Longitudinal Data Analysis', 'Sentiment Analysis'], 'soft_skills': [], 'tools_with_levels': [{'tool': 'SQL', 'level': '-'}, {'tool': 'Hadoop', 'level': '-'}, {'tool': 'AWS', 'level': '-'}, {'tool': 'Tableau', 'level': '-'}, {'tool': 'Matplotlib', 'level': '-'}, {'tool': 'WordCloud', 'level': '-'}, {'tool': 'Power BI', 'level': '-'}, {'tool': 'Snowflake', 'level': '-'}], 'languages': []}}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uLPwleGahXh",
        "outputId": "5057886f-2870-4457-f13b-d28ebd22ff98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"experience_relevance_score\": 28,\n",
            "  \"explanation\": [\n",
            "    \"Strong proficiency in Python and SQL, with demonstrated experience in data cleaning, preprocessing, and feature engineering.\",\n",
            "    \"Extensive experience in building and evaluating various machine learning models (SVM, Random Forest, Decision Tree, Logistic Regression) and NLP techniques (classification, sentiment analysis, text feature extraction).\",\n",
            "    \"Good alignment with analyzing data for insights, proposing solutions, and presenting findings to stakeholders, utilizing tools like Matplotlib and Tableau (Power BI listed as a skill).\",\n",
            "    \"Significant gaps in hands-on experience with PySpark and cloud-based big data platforms (e.g., Databricks, BigQuery, Redshift, or detailed Hadoop experience) which are key requirements for end-to-end data pipeline design and migration.\",\n",
            "    \"Limited explicit experience in designing and maintaining production-level data pipelines and deploying models beyond local serialization.\"\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_experience_relevance = json.loads(response.text.split('```json')[1].split('```')[0])\n",
        "output_experience_relevance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVkr4KEdawVT",
        "outputId": "551ff5d0-5430-4015-ad3b-e5d4ea8695b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'experience_relevance_score': 28,\n",
              " 'explanation': ['Strong proficiency in Python and SQL, with demonstrated experience in data cleaning, preprocessing, and feature engineering.',\n",
              "  'Extensive experience in building and evaluating various machine learning models (SVM, Random Forest, Decision Tree, Logistic Regression) and NLP techniques (classification, sentiment analysis, text feature extraction).',\n",
              "  'Good alignment with analyzing data for insights, proposing solutions, and presenting findings to stakeholders, utilizing tools like Matplotlib and Tableau (Power BI listed as a skill).',\n",
              "  'Significant gaps in hands-on experience with PySpark and cloud-based big data platforms (e.g., Databricks, BigQuery, Redshift, or detailed Hadoop experience) which are key requirements for end-to-end data pipeline design and migration.',\n",
              "  'Limited explicit experience in designing and maintaining production-level data pipelines and deploying models beyond local serialization.']}"
            ]
          },
          "metadata": {},
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "point_experience_relevancee = output_experience_relevance[\"experience_relevance_score\"]\n",
        "point_experience_relevancee"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-wluE3fa5IQ",
        "outputId": "92369461-b838-4d2b-e265-a02350842fb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {},
          "execution_count": 297
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "point_role_relevance = point_skill_match+point_experience_relevancee\n",
        "point_role_relevance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl-P-G1Pa878",
        "outputId": "9ac34a12-4a9b-4b3b-e857-af36b7463254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38.326086956521735"
            ]
          },
          "metadata": {},
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>"
      ],
      "metadata": {
        "id": "oCidFJSUXGzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Section-Level_criteria"
      ],
      "metadata": {
        "id": "XJ9R6343T7GU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are an expert resume parser and HR data structurer.\n",
        "\n",
        "Your goal is to extract and normalize resume data into a structured JSON schema.\n",
        "\n",
        "From the given resume text below, fill out every field in the schema.\n",
        "If a field does not exist in the resume, fill it with a single dash (\"-\").\n",
        "\n",
        "Return only valid JSON — no explanations, no extra text.\n",
        "\n",
        "JSON Schema:\n",
        "{{\n",
        "  \"contact_information\": {{\n",
        "    \"name\": \"\",\n",
        "    \"email\": \"\",\n",
        "    \"phone\": \"\",\n",
        "    \"linkedin\": \"\",\n",
        "    \"jobdb_link\": \"\",\n",
        "    \"portfolio_link\": \"\"\n",
        "  }},\n",
        "  \"professional_summary\": {{\n",
        "    \"has_summary\": \"\",\n",
        "    \"summary_points\": []\n",
        "  }},\n",
        "  \"education\": [\n",
        "    {{\n",
        "      \"institution\": \"\",\n",
        "      \"degree\": \"\",\n",
        "      \"dates\": \"\",\n",
        "      \"gpa\": \"\",\n",
        "      \"honors\": \"\"\n",
        "    }}\n",
        "  ],\n",
        "  \"experience\": [],\n",
        "  \"skills\": {{\n",
        "    \"technical\": [],\n",
        "    \"soft_skills\": [],\n",
        "    \"tools_with_levels\": [\n",
        "      {{\"tool\": \"\", \"level\": \"\"}}\n",
        "    ],\n",
        "    \"languages\": [\n",
        "      {{\"language\": \"\", \"level\": \"\"}}\n",
        "    ]\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Rules:\n",
        "- If data not found, put \"-\"\n",
        "- Return strictly in JSON format only\n",
        "- For professional_summary, set \"has_summary\" = \"Yes\" if a summary paragraph exists, otherwise \"No\"\n",
        "- Split bullet-like sentences into array elements\n",
        "\n",
        "Resume text:\n",
        "{resume_text}\n",
        "\"\"\"\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXAhlznMT44s",
        "outputId": "8ad95f0e-6d35-405b-c852-ee87a9721a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"contact_information\": {\n",
            "    \"name\": \"Surya Teja Menta\",\n",
            "    \"email\": \"-\",\n",
            "    \"phone\": \"+91 8309584461\",\n",
            "    \"linkedin\": \"Surya-Teja-Menta\",\n",
            "    \"jobdb_link\": \"-\",\n",
            "    \"portfolio_link\": \"suryatejamenta.co.in\"\n",
            "  },\n",
            "  \"professional_summary\": {\n",
            "    \"has_summary\": \"Yes\",\n",
            "    \"summary_points\": [\n",
            "      \"I’m Surya Teja Menta, Results-driven Senior Data Scientist with 4+ years of experience in Data Science, Machine Learning (ML), and Generative AI (GenAI).\",\n",
            "      \"Proven expertise in RAG (Retrieval-Augmented Generation), LLM fine-tuning, MLOps, and end-to-end AI solutions.\",\n",
            "      \"Strong background in data analytics, statistical modeling, AI-powered automation, and scalable AI architectures.\",\n",
            "      \"IBM Certified Professional Data Scientist with hands-on experience in LangChain, Hugging Face, OpenAI APIs, Vector Databases (ChromaDB, Pinecone), and cloud deployments (AWS, GCP).\",\n",
            "      \"Passionate about AI research, model optimization, and developing cutting-edge AI solutions.\"\n",
            "    ]\n",
            "  },\n",
            "  \"education\": [\n",
            "    {\n",
            "      \"institution\": \"PBR VITS, Kavali, AP\",\n",
            "      \"degree\": \"Bachelor's Degree(CS)\",\n",
            "      \"dates\": \"June 2016 - May 2020\",\n",
            "      \"gpa\": \"78.3%\",\n",
            "      \"honors\": \"Participated in paper presentations and won prizes.\"\n",
            "    },\n",
            "    {\n",
            "      \"institution\": \"Narayana Junior College, Kavali, AP\",\n",
            "      \"degree\": \"HSC\",\n",
            "      \"dates\": \"June 2014 - May 2016\",\n",
            "      \"gpa\": \"92.5%\",\n",
            "      \"honors\": \"-\"\n",
            "    },\n",
            "    {\n",
            "      \"institution\": \"Kranthi EM School, Kavali, AP\",\n",
            "      \"degree\": \"SSC\",\n",
            "      \"dates\": \"June 2004 - May 2014\",\n",
            "      \"gpa\": \"87%\",\n",
            "      \"honors\": \"Participated in school dramas, sports, etc.\"\n",
            "    }\n",
            "  ],\n",
            "  \"experience\": [\n",
            "    {\n",
            "      \"title\": \"Senior Data Scientist\",\n",
            "      \"company\": \"Robert Bosch\",\n",
            "      \"dates\": \"Jan 2024 - preset\",\n",
            "      \"description\": [\n",
            "        \"Designed and deployed custom YOLOv8 models for computer vision applications.\",\n",
            "        \"Built multi-modal RAG pipelines for intelligent document processing, contextual retrieval, and GenAI-powered insights.\",\n",
            "        \"Worked on Time series data for Adnoc Project.\",\n",
            "        \"Developed ML algorithms for signal processing and sensor vibration data analytics using scikit-learn, PyTorch, and TensorFlow.\",\n",
            "        \"Worked on LLM fine-tuning, prompt engineering, and efficient model inference for enterprise AI applications.\",\n",
            "        \"Integrated MLOps best practices for scalable AI pipelines, model versioning, and automated deployment.\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Subject Matter Expert (Data Analyst)\",\n",
            "      \"company\": \"Tudip Technologies\",\n",
            "      \"dates\": \"Oct 2020 - Oct 2023\",\n",
            "      \"description\": [\n",
            "        \"Developed interactive analytics dashboards and data reports in Google Data Studio for content management analytics.\",\n",
            "        \"Automated text summarization & paraphrasing using NLP Transformers (BART, T5) and deployed on Google Cloud Run.\",\n",
            "        \"Built AI-powered data visualization tools to enhance real-time business intelligence and trend analysis.\",\n",
            "        \"Integrated LLM-powered chatbots with Google Sheets API to improve data accessibility across teams.\",\n",
            "        \"Recognized with the Best Team Award for highest client appreciation and impactful AI-driven automation.\"\n",
            "      ]\n",
            "    }\n",
            "  ],\n",
            "  \"skills\": {\n",
            "    \"technical\": [\n",
            "      \"Data Science\",\n",
            "      \"Machine Learning\",\n",
            "      \"Deep Learning\",\n",
            "      \"Natural Language Processing (NLP)\",\n",
            "      \"Computer Vision (CV)\",\n",
            "      \"Large Language Models (LLMs)\",\n",
            "      \"LLM Fine Tuning\",\n",
            "      \"Generative AI (GenAI)\",\n",
            "      \"Retrieval-Augmented Generation (RAG)\",\n",
            "      \"MLOps\",\n",
            "      \"Prompt Engineering\",\n",
            "      \"Model Optimization\",\n",
            "      \"Model Inference\",\n",
            "      \"Object Detection\",\n",
            "      \"Image Processing\",\n",
            "      \"Signal Processing\",\n",
            "      \"Time Series Analysis\",\n",
            "      \"Vector Databases\",\n",
            "      \"Semantic Search\",\n",
            "      \"Automated Complexity Analysis\",\n",
            "      \"Memory Optimization\",\n",
            "      \"Complexity Scoring\",\n",
            "      \"Statistical Modeling\",\n",
            "      \"Data Analytics\",\n",
            "      \"Data Mining\",\n",
            "      \"Hypothesis Testing\",\n",
            "      \"Model Building\",\n",
            "      \"Model Deployment\",\n",
            "      \"YOLOv8\",\n",
            "      \"GANs\",\n",
            "      \"Transformers (BART, T5)\",\n",
            "      \"Scikit-learn\",\n",
            "      \"Numpy\",\n",
            "      \"Pandas\",\n",
            "      \"Spacy\",\n",
            "      \"NLTK\",\n",
            "      \"OpenCV\",\n",
            "      \"Matplotlib\",\n",
            "      \"Seaborn\",\n",
            "      \"Scipy\",\n",
            "      \"Tensorflow\",\n",
            "      \"PyTorch\",\n",
            "      \"Keras\",\n",
            "      \"LangChain\",\n",
            "      \"IBM Certified Professional Data Scientist\",\n",
            "      \"AWS\",\n",
            "      \"GCP\",\n",
            "      \"Chainlit\",\n",
            "      \"ESRGAN\",\n",
            "      \"BM25 Retrieval\",\n",
            "      \"Multi-modal PDF Processing\",\n",
            "      \"Semantic Chunking\",\n",
            "      \"Adaptive Query Handling\",\n",
            "      \"Contextual Response Generation\",\n",
            "      \"Context-Tracking Algorithms\",\n",
            "      \"Real-time Business Intelligence\",\n",
            "      \"Trend Analysis\",\n",
            "      \"AI-powered Automation\",\n",
            "      \"Scalable AI Architectures\"\n",
            "    ],\n",
            "    \"soft_skills\": [\n",
            "      \"Analytical Thinking\",\n",
            "      \"Problem Solving\",\n",
            "      \"Collaboration\",\n",
            "      \"Communication\",\n",
            "      \"Continuous Learning\",\n",
            "      \"Adaptability\",\n",
            "      \"Time Management\"\n",
            "    ],\n",
            "    \"tools_with_levels\": [\n",
            "      {\n",
            "        \"tool\": \"Jupyter notebooks\",\n",
            "        \"level\": \"-\"\n",
            "      },\n",
            "      {\n",
            "        \"tool\": \"Docker\",\n",
            "        \"level\": \"-\"\n",
            "      },\n",
            "      {\n",
            "        \"tool\": \"GitHub\",\n",
            "        \"level\": \"-\"\n",
            "      },\n",
            "      {\n",
            "        \"tool\": \"Git\",\n",
            "        \"level\": \"-\"\n",
            "      },\n",
            "      {\n",
            "        \"tool\": \"Google Data Studio\",\n",
            "        \"level\": \"-\"\n",
            "      },\n",
            "      {\n",
            "        \"tool\": \"Google Cloud Run\",\n",
            "        \"level\": \"-\"\n",
            "      },\n",
            "      {\n",
            "        \"tool\": \"Amazon SageMaker\",\n",
            "        \"level\": \"basic\"\n",
            "      },\n",
            "      {\n",
            "        \"tool\": \"ChromaDB\",\n",
            "        \"level\": \"-\"\n",
            "      },\n",
            "      {\n",
            "        \"tool\": \"Pinecone\",\n",
            "        \"level\": \"-\"\n",
            "      },\n",
            "      {\n",
            "        \"tool\": \"OpenAI APIs\",\n",
            "        \"level\": \"-\"\n",
            "      },\n",
            "      {\n",
            "        \"tool\": \"Hugging Face\",\n",
            "        \"level\": \"-\"\n",
            "      }\n",
            "    ],\n",
            "    \"languages\": [\n",
            "      {\n",
            "        \"language\": \"Python\",\n",
            "        \"level\": \"-\"\n",
            "      },\n",
            "      {\n",
            "        \"language\": \"SQL\",\n",
            "        \"level\": \"-\"\n",
            "      },\n",
            "      {\n",
            "        \"language\": \"HTML\",\n",
            "        \"level\": \"-\"\n",
            "      },\n",
            "      {\n",
            "        \"language\": \"CSS\",\n",
            "        \"level\": \"-\"\n",
            "      },\n",
            "      {\n",
            "        \"language\": \"NoSQL\",\n",
            "        \"level\": \"-\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_json = json.loads(response.text.split('```json')[1].split('```')[0])\n",
        "resume_json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3sC31O2VLp6",
        "outputId": "c9bb124f-048c-4d80-ca28-4572849ba105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'contact_information': {'name': 'Surya Teja Menta',\n",
              "  'email': '-',\n",
              "  'phone': '+91 8309584461',\n",
              "  'linkedin': 'Surya-Teja-Menta',\n",
              "  'jobdb_link': '-',\n",
              "  'portfolio_link': 'suryatejamenta.co.in'},\n",
              " 'professional_summary': {'has_summary': 'Yes',\n",
              "  'summary_points': ['I’m Surya Teja Menta, Results-driven Senior Data Scientist with 4+ years of experience in Data Science, Machine Learning (ML), and Generative AI (GenAI).',\n",
              "   'Proven expertise in RAG (Retrieval-Augmented Generation), LLM fine-tuning, MLOps, and end-to-end AI solutions.',\n",
              "   'Strong background in data analytics, statistical modeling, AI-powered automation, and scalable AI architectures.',\n",
              "   'IBM Certified Professional Data Scientist with hands-on experience in LangChain, Hugging Face, OpenAI APIs, Vector Databases (ChromaDB, Pinecone), and cloud deployments (AWS, GCP).',\n",
              "   'Passionate about AI research, model optimization, and developing cutting-edge AI solutions.']},\n",
              " 'education': [{'institution': 'PBR VITS, Kavali, AP',\n",
              "   'degree': \"Bachelor's Degree(CS)\",\n",
              "   'dates': 'June 2016 - May 2020',\n",
              "   'gpa': '78.3%',\n",
              "   'honors': 'Participated in paper presentations and won prizes.'},\n",
              "  {'institution': 'Narayana Junior College, Kavali, AP',\n",
              "   'degree': 'HSC',\n",
              "   'dates': 'June 2014 - May 2016',\n",
              "   'gpa': '92.5%',\n",
              "   'honors': '-'},\n",
              "  {'institution': 'Kranthi EM School, Kavali, AP',\n",
              "   'degree': 'SSC',\n",
              "   'dates': 'June 2004 - May 2014',\n",
              "   'gpa': '87%',\n",
              "   'honors': 'Participated in school dramas, sports, etc.'}],\n",
              " 'experience': [{'title': 'Senior Data Scientist',\n",
              "   'company': 'Robert Bosch',\n",
              "   'dates': 'Jan 2024 - preset',\n",
              "   'description': ['Designed and deployed custom YOLOv8 models for computer vision applications.',\n",
              "    'Built multi-modal RAG pipelines for intelligent document processing, contextual retrieval, and GenAI-powered insights.',\n",
              "    'Worked on Time series data for Adnoc Project.',\n",
              "    'Developed ML algorithms for signal processing and sensor vibration data analytics using scikit-learn, PyTorch, and TensorFlow.',\n",
              "    'Worked on LLM fine-tuning, prompt engineering, and efficient model inference for enterprise AI applications.',\n",
              "    'Integrated MLOps best practices for scalable AI pipelines, model versioning, and automated deployment.']},\n",
              "  {'title': 'Subject Matter Expert (Data Analyst)',\n",
              "   'company': 'Tudip Technologies',\n",
              "   'dates': 'Oct 2020 - Oct 2023',\n",
              "   'description': ['Developed interactive analytics dashboards and data reports in Google Data Studio for content management analytics.',\n",
              "    'Automated text summarization & paraphrasing using NLP Transformers (BART, T5) and deployed on Google Cloud Run.',\n",
              "    'Built AI-powered data visualization tools to enhance real-time business intelligence and trend analysis.',\n",
              "    'Integrated LLM-powered chatbots with Google Sheets API to improve data accessibility across teams.',\n",
              "    'Recognized with the Best Team Award for highest client appreciation and impactful AI-driven automation.']}],\n",
              " 'skills': {'technical': ['Data Science',\n",
              "   'Machine Learning',\n",
              "   'Deep Learning',\n",
              "   'Natural Language Processing (NLP)',\n",
              "   'Computer Vision (CV)',\n",
              "   'Large Language Models (LLMs)',\n",
              "   'LLM Fine Tuning',\n",
              "   'Generative AI (GenAI)',\n",
              "   'Retrieval-Augmented Generation (RAG)',\n",
              "   'MLOps',\n",
              "   'Prompt Engineering',\n",
              "   'Model Optimization',\n",
              "   'Model Inference',\n",
              "   'Object Detection',\n",
              "   'Image Processing',\n",
              "   'Signal Processing',\n",
              "   'Time Series Analysis',\n",
              "   'Vector Databases',\n",
              "   'Semantic Search',\n",
              "   'Automated Complexity Analysis',\n",
              "   'Memory Optimization',\n",
              "   'Complexity Scoring',\n",
              "   'Statistical Modeling',\n",
              "   'Data Analytics',\n",
              "   'Data Mining',\n",
              "   'Hypothesis Testing',\n",
              "   'Model Building',\n",
              "   'Model Deployment',\n",
              "   'YOLOv8',\n",
              "   'GANs',\n",
              "   'Transformers (BART, T5)',\n",
              "   'Scikit-learn',\n",
              "   'Numpy',\n",
              "   'Pandas',\n",
              "   'Spacy',\n",
              "   'NLTK',\n",
              "   'OpenCV',\n",
              "   'Matplotlib',\n",
              "   'Seaborn',\n",
              "   'Scipy',\n",
              "   'Tensorflow',\n",
              "   'PyTorch',\n",
              "   'Keras',\n",
              "   'LangChain',\n",
              "   'IBM Certified Professional Data Scientist',\n",
              "   'AWS',\n",
              "   'GCP',\n",
              "   'Chainlit',\n",
              "   'ESRGAN',\n",
              "   'BM25 Retrieval',\n",
              "   'Multi-modal PDF Processing',\n",
              "   'Semantic Chunking',\n",
              "   'Adaptive Query Handling',\n",
              "   'Contextual Response Generation',\n",
              "   'Context-Tracking Algorithms',\n",
              "   'Real-time Business Intelligence',\n",
              "   'Trend Analysis',\n",
              "   'AI-powered Automation',\n",
              "   'Scalable AI Architectures'],\n",
              "  'soft_skills': ['Analytical Thinking',\n",
              "   'Problem Solving',\n",
              "   'Collaboration',\n",
              "   'Communication',\n",
              "   'Continuous Learning',\n",
              "   'Adaptability',\n",
              "   'Time Management'],\n",
              "  'tools_with_levels': [{'tool': 'Jupyter notebooks', 'level': '-'},\n",
              "   {'tool': 'Docker', 'level': '-'},\n",
              "   {'tool': 'GitHub', 'level': '-'},\n",
              "   {'tool': 'Git', 'level': '-'},\n",
              "   {'tool': 'Google Data Studio', 'level': '-'},\n",
              "   {'tool': 'Google Cloud Run', 'level': '-'},\n",
              "   {'tool': 'Amazon SageMaker', 'level': 'basic'},\n",
              "   {'tool': 'ChromaDB', 'level': '-'},\n",
              "   {'tool': 'Pinecone', 'level': '-'},\n",
              "   {'tool': 'OpenAI APIs', 'level': '-'},\n",
              "   {'tool': 'Hugging Face', 'level': '-'}],\n",
              "  'languages': [{'language': 'Python', 'level': '-'},\n",
              "   {'language': 'SQL', 'level': '-'},\n",
              "   {'language': 'HTML', 'level': '-'},\n",
              "   {'language': 'CSS', 'level': '-'},\n",
              "   {'language': 'NoSQL', 'level': '-'}]}}"
            ]
          },
          "metadata": {},
          "execution_count": 300
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are an expert HR evaluator and resume reviewer.\n",
        "\n",
        "You will receive structured resume data in JSON format.\n",
        "Your task is to assign a completeness score (0–20) for each of the five major sections.\n",
        "\n",
        "### Sections to evaluate:\n",
        "1. **Contact Information**\n",
        "2. **Professional Summary**\n",
        "3. **Education**\n",
        "4. **Experience**\n",
        "5. **Skills**\n",
        "\n",
        "### Scoring Guide (0–20)\n",
        "- 0–5: Missing or minimal content\n",
        "- 6–10: Partial information (some fields empty or vague)\n",
        "- 11–15: Mostly complete with moderate detail\n",
        "- 16–20: Fully complete and rich in information (well-detailed, clear, professional)\n",
        "\n",
        "### Evaluation Rules\n",
        "- Evaluate **content richness, completeness, and clarity**.\n",
        "- Missing fields or dashes (“-”) lower the score.\n",
        "- Count variety and depth (e.g., multiple education entries or skill levels add points).\n",
        "- Experience should reward both quantity and quality (e.g., clear responsibilities, bullet points).\n",
        "- Skills section should reward technical + soft skills + tools with levels.\n",
        "\n",
        "### Return format (JSON only)\n",
        "{{\n",
        "  \"structure_score\": {{\n",
        "    \"contact_information\": {{\n",
        "      \"score\": <0-20>,\n",
        "      \"explanation\": \"\"\n",
        "    }},\n",
        "    \"professional_summary\": {{\n",
        "      \"score\": <0-20>,\n",
        "      \"explanation\": \"\"\n",
        "    }},\n",
        "    \"education\": {{\n",
        "      \"score\": <0-20>,\n",
        "      \"explanation\": \"\"\n",
        "    }},\n",
        "    \"experience\": {{\n",
        "      \"score\": <0-20>,\n",
        "      \"explanation\": \"\"\n",
        "    }},\n",
        "    \"skills\": {{\n",
        "      \"score\": <0-20>,\n",
        "      \"explanation\": \"\"\n",
        "    }},\n",
        "    \"total_score\": <0-100>\n",
        "  }}\n",
        "}}\n",
        "\n",
        "### Resume Data ###\n",
        "{resume_json}\n",
        "\"\"\"\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt\n",
        ")"
      ],
      "metadata": {
        "id": "g9s4HTxLU3sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Output_Sectionlevelcriteria = json.loads(response.text.split('```json')[1].split('```')[0])\n",
        "print(Output_Sectionlevelcriteria)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ5BCy0RVxTn",
        "outputId": "387707c8-11d1-4c5d-9e6c-870a832882a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'structure_score': {'contact_information': {'score': 12, 'explanation': \"The contact information includes name, phone, LinkedIn, and a portfolio link. However, the email address and jobdb_link are marked with a dash ('-'), indicating missing critical information, which lowers the completeness score significantly.\"}, 'professional_summary': {'score': 19, 'explanation': 'The professional summary is exceptionally well-written and comprehensive. It consists of five detailed points clearly outlining years of experience, key technical expertise (e.g., RAG, LLM fine-tuning, MLOps, GenAI), certifications (IBM Certified), specific tools (LangChain, Hugging Face, vector databases), and career passion. It is rich in relevant keywords and provides a strong overview.'}, 'education': {'score': 19, 'explanation': \"The education section is fully complete and detailed, featuring three entries (Bachelor's, HSC, SSC). Each entry includes the institution, degree, dates, and GPA. Relevant honors and participation details are also provided, making this section very thorough and clear.\"}, 'experience': {'score': 19, 'explanation': 'The experience section is very strong, detailing two roles with clear titles, companies, and dates. Each position includes multiple, well-articulated bullet points that describe responsibilities and quantifiable achievements, highlighting specific technologies (YOLOv8, RAG, NLP Transformers) and business impact (client appreciation, AI-driven automation). It demonstrates both depth and quality of work.'}, 'skills': {'score': 14, 'explanation': \"The skills section is rich in variety, listing an extensive array of technical skills across Data Science, ML, DL, NLP, CV, LLMs, and MLOps, along with a good list of soft skills. However, the 'tools_with_levels' and 'languages' sub-sections are incomplete regarding proficiency levels; most entries are marked with a dash ('-'), indicating a lack of detail. Only one tool has a specified level ('basic'). This significantly reduces the completeness score for depth, despite the broad range of skills listed.\"}, 'total_score': 83}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gymxM4UKV_oq",
        "outputId": "88052f92-000e-4b53-f718-55f02647181e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"structure_score\": {\n",
            "    \"contact_information\": {\n",
            "      \"score\": 12,\n",
            "      \"explanation\": \"The contact information includes name, phone, LinkedIn, and a portfolio link. However, the email address and jobdb_link are marked with a dash ('-'), indicating missing critical information, which lowers the completeness score significantly.\"\n",
            "    },\n",
            "    \"professional_summary\": {\n",
            "      \"score\": 19,\n",
            "      \"explanation\": \"The professional summary is exceptionally well-written and comprehensive. It consists of five detailed points clearly outlining years of experience, key technical expertise (e.g., RAG, LLM fine-tuning, MLOps, GenAI), certifications (IBM Certified), specific tools (LangChain, Hugging Face, vector databases), and career passion. It is rich in relevant keywords and provides a strong overview.\"\n",
            "    },\n",
            "    \"education\": {\n",
            "      \"score\": 19,\n",
            "      \"explanation\": \"The education section is fully complete and detailed, featuring three entries (Bachelor's, HSC, SSC). Each entry includes the institution, degree, dates, and GPA. Relevant honors and participation details are also provided, making this section very thorough and clear.\"\n",
            "    },\n",
            "    \"experience\": {\n",
            "      \"score\": 19,\n",
            "      \"explanation\": \"The experience section is very strong, detailing two roles with clear titles, companies, and dates. Each position includes multiple, well-articulated bullet points that describe responsibilities and quantifiable achievements, highlighting specific technologies (YOLOv8, RAG, NLP Transformers) and business impact (client appreciation, AI-driven automation). It demonstrates both depth and quality of work.\"\n",
            "    },\n",
            "    \"skills\": {\n",
            "      \"score\": 14,\n",
            "      \"explanation\": \"The skills section is rich in variety, listing an extensive array of technical skills across Data Science, ML, DL, NLP, CV, LLMs, and MLOps, along with a good list of soft skills. However, the 'tools_with_levels' and 'languages' sub-sections are incomplete regarding proficiency levels; most entries are marked with a dash ('-'), indicating a lack of detail. Only one tool has a specified level ('basic'). This significantly reduces the completeness score for depth, despite the broad range of skills listed.\"\n",
            "    },\n",
            "    \"total_score\": 83\n",
            "  }\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "point_section_level_criteria = \\\n",
        "Output_Sectionlevelcriteria[\"structure_score\"][\"contact_information\"][\"score\"]+\\\n",
        "Output_Sectionlevelcriteria[\"structure_score\"][\"professional_summary\"][\"score\"]+\\\n",
        "Output_Sectionlevelcriteria[\"structure_score\"][\"education\"][\"score\"]+\\\n",
        "Output_Sectionlevelcriteria[\"structure_score\"][\"experience\"][\"score\"]+\\\n",
        "Output_Sectionlevelcriteria[\"structure_score\"][\"skills\"][\"score\"]\n",
        "point_section_level_criteria"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6RGDyY7WxCV",
        "outputId": "7ceb443a-d9e6-43c3-ea7d-1d9dae70c4bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "metadata": {},
          "execution_count": 304
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>"
      ],
      "metadata": {
        "id": "j8_PC1wRWwQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Content_quality"
      ],
      "metadata": {
        "id": "KFIBUtBccDsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are an expert data career evaluator and resume reviewer.\n",
        "\n",
        "Evaluate the following resume content for **Content Quality**, focusing on five dimensions.\n",
        "For each dimension, provide:\n",
        "- A numeric score (0–20)\n",
        "- A brief explanation (2–3 sentences)\n",
        "- 1–2 improvement suggestions\n",
        "\n",
        "### Scoring Guide\n",
        "- 0–5: Poor (mostly missing or irrelevant)\n",
        "- 6–10: Fair (partially meets expectations)\n",
        "- 11–15: Good (clear but needs more depth)\n",
        "- 16–20: Excellent (strong and well-written)\n",
        "\n",
        "### Evaluation Dimensions\n",
        "1. **Quantifiable Impact**\n",
        "   - Measures if achievements include measurable metrics (accuracy, F1-score, ROI, cost reduction, etc.)\n",
        "   - Every bullet should ideally include numbers.\n",
        "\n",
        "2. **Skill Demonstration**\n",
        "   - Evaluates whether each bullet shows what skill, tool, or technology was applied.\n",
        "   - e.g., “Built face recognition system using Python and AWS”.\n",
        "\n",
        "3. **Specificity & Relevance**\n",
        "   - Checks if statements are specific and relevant to the field.\n",
        "   - Avoid vague phrases like “worked on many projects”; prefer “developed a fraud detection model using Random Forest”.\n",
        "\n",
        "4. **Grammar & Language Use**\n",
        "   - Evaluates sentence clarity, grammar, and professional tone.\n",
        "   - Prefer strong action verbs such as “Led”, “Designed”, “Implemented”.\n",
        "\n",
        "5. **Active Writing Style**\n",
        "   - Checks for active-voice writing instead of passive.\n",
        "   - e.g., “Created predictive model” ✅ vs “Was responsible for creating” ❌.\n",
        "\n",
        "### Return Format (JSON only)\n",
        "{{\n",
        "  \"content_quality\": {{\n",
        "    \"quantifiable_impact\": {{\n",
        "      \"score\": <0-20>,\n",
        "      \"explanation\": \"\",\n",
        "      \"suggestions\": []\n",
        "    }},\n",
        "    \"skill_demonstration\": {{\n",
        "      \"score\": <0-20>,\n",
        "      \"explanation\": \"\",\n",
        "      \"suggestions\": []\n",
        "    }},\n",
        "    \"specificity_and_relevance\": {{\n",
        "      \"score\": <0-20>,\n",
        "      \"explanation\": \"\",\n",
        "      \"suggestions\": []\n",
        "    }},\n",
        "    \"grammar_and_language_use\": {{\n",
        "      \"score\": <0-20>,\n",
        "      \"explanation\": \"\",\n",
        "      \"suggestions\": []\n",
        "    }},\n",
        "    \"active_writing_style\": {{\n",
        "      \"score\": <0-20>,\n",
        "      \"explanation\": \"\",\n",
        "      \"suggestions\": []\n",
        "    }},\n",
        "    \"total_score\": <0-100>\n",
        "  }}\n",
        "}}\n",
        "\n",
        "### Notes\n",
        "- Return only valid JSON.\n",
        "- Deduct points for vague wording or lack of measurable data.\n",
        "- Evaluate based on English and Thai if both appear.\n",
        "\n",
        "### RESUME CONTENT ###\n",
        "{resume_json}\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt\n",
        ")"
      ],
      "metadata": {
        "id": "GOT0lJQCcH-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36Cel_aQq1fa",
        "outputId": "271b5c78-ec6a-4590-e90e-32313ec1294f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"content_quality\": {\n",
            "    \"quantifiable_impact\": {\n",
            "      \"score\": 8,\n",
            "      \"explanation\": \"While the professional summary mentions 4+ years of experience and one bullet highlights being 'Recognized with the Best Team Award,' the vast majority of achievement bullets lack specific metrics or quantifiable outcomes. This omission makes it difficult to assess the scale, success, or business value of the projects and contributions.\",\n",
            "      \"suggestions\": [\n",
            "        \"Integrate specific metrics (e.g., accuracy improvements, efficiency gains, cost reductions, number of users/teams impacted, percentage increases) into each achievement bullet to clearly quantify the impact of the work performed.\",\n",
            "        \"For the 'Best Team Award,' elaborate on the measurable results or achievements of the AI-driven automation that led to the recognition.\"\n",
            "      ]\n",
            "    },\n",
            "    \"skill_demonstration\": {\n",
            "      \"score\": 17,\n",
            "      \"explanation\": \"The resume effectively showcases a broad and relevant skill set, clearly highlighting the technologies (e.g., YOLOv8, PyTorch, Transformers, Google Data Studio) and methodologies (e.g., RAG, MLOps, LLM fine-tuning) applied within most bullet points. This demonstrates a strong technical foundation aligned with data science and AI roles.\",\n",
            "      \"suggestions\": [\n",
            "        \"Replace generic phrases like 'Worked on' with stronger action verbs that specifically describe the application of a skill (e.g., 'Analyzed time series data,' 'Optimized LLM fine-tuning').\",\n",
            "        \"For the Adnoc project, briefly mentioning the objective or problem addressed would further contextualize the skill demonstration.\"\n",
            "      ]\n",
            "    },\n",
            "    \"specificity_and_relevance\": {\n",
            "      \"score\": 15,\n",
            "      \"explanation\": \"The content is largely specific, detailing technologies (e.g., YOLOv8 models, NLP Transformers, LLM-powered chatbots) and relevant project types (computer vision, RAG pipelines, MLOps). This avoids vague generic statements and strongly aligns with the target career field of data science and AI.\",\n",
            "      \"suggestions\": [\n",
            "        \"Enhance the two 'Worked on' bullets by providing more precise details about the nature of the work and its specific outcomes for greater clarity and impact.\",\n",
            "        \"Where possible, briefly explain the 'why' behind specific projects or deployments to further establish relevance and the problem-solving approach.\"\n",
            "      ]\n",
            "    },\n",
            "    \"grammar_and_language_use\": {\n",
            "      \"score\": 18,\n",
            "      \"explanation\": \"The resume exhibits excellent grammar and syntax, with a professional tone maintained throughout. Strong action verbs are used consistently at the beginning of most bullet points, contributing to clear and concise communication.\",\n",
            "      \"suggestions\": [\n",
            "        \"Review the professional summary for potential opportunities to consolidate ideas for even greater conciseness, while retaining impact.\",\n",
            "        \"Ensure consistent capitalization for all technical terms and proper nouns throughout the document.\"\n",
            "      ]\n",
            "    },\n",
            "    \"active_writing_style\": {\n",
            "      \"score\": 17,\n",
            "      \"explanation\": \"The resume predominantly uses an active voice, starting most achievement bullets with powerful verbs such as 'Designed,' 'Built,' 'Developed,' and 'Automated.' This effectively conveys the candidate's direct involvement and contributions to the projects, making the experience sections impactful.\",\n",
            "      \"suggestions\": [\n",
            "        \"Convert the remaining passive or less active phrases, such as 'Worked on' and 'Recognized with,' into active constructions to reinforce personal agency and direct impact.\",\n",
            "        \"Prioritize action verbs that precisely reflect the candidate's role and contribution in each statement, e.g., 'Led the integration of MLOps best practices...'.\"\n",
            "      ]\n",
            "    },\n",
            "    \"total_score\": 75\n",
            "  }\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Output_content_quality = json.loads(response.text.split('```json')[1].split('```')[0])\n",
        "Output_content_quality"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abjxqOrlqUUB",
        "outputId": "c39d5736-0065-4ca4-d246-be31519b0855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'content_quality': {'quantifiable_impact': {'score': 8,\n",
              "   'explanation': \"While the professional summary mentions 4+ years of experience and one bullet highlights being 'Recognized with the Best Team Award,' the vast majority of achievement bullets lack specific metrics or quantifiable outcomes. This omission makes it difficult to assess the scale, success, or business value of the projects and contributions.\",\n",
              "   'suggestions': ['Integrate specific metrics (e.g., accuracy improvements, efficiency gains, cost reductions, number of users/teams impacted, percentage increases) into each achievement bullet to clearly quantify the impact of the work performed.',\n",
              "    \"For the 'Best Team Award,' elaborate on the measurable results or achievements of the AI-driven automation that led to the recognition.\"]},\n",
              "  'skill_demonstration': {'score': 17,\n",
              "   'explanation': 'The resume effectively showcases a broad and relevant skill set, clearly highlighting the technologies (e.g., YOLOv8, PyTorch, Transformers, Google Data Studio) and methodologies (e.g., RAG, MLOps, LLM fine-tuning) applied within most bullet points. This demonstrates a strong technical foundation aligned with data science and AI roles.',\n",
              "   'suggestions': [\"Replace generic phrases like 'Worked on' with stronger action verbs that specifically describe the application of a skill (e.g., 'Analyzed time series data,' 'Optimized LLM fine-tuning').\",\n",
              "    'For the Adnoc project, briefly mentioning the objective or problem addressed would further contextualize the skill demonstration.']},\n",
              "  'specificity_and_relevance': {'score': 15,\n",
              "   'explanation': 'The content is largely specific, detailing technologies (e.g., YOLOv8 models, NLP Transformers, LLM-powered chatbots) and relevant project types (computer vision, RAG pipelines, MLOps). This avoids vague generic statements and strongly aligns with the target career field of data science and AI.',\n",
              "   'suggestions': [\"Enhance the two 'Worked on' bullets by providing more precise details about the nature of the work and its specific outcomes for greater clarity and impact.\",\n",
              "    \"Where possible, briefly explain the 'why' behind specific projects or deployments to further establish relevance and the problem-solving approach.\"]},\n",
              "  'grammar_and_language_use': {'score': 18,\n",
              "   'explanation': 'The resume exhibits excellent grammar and syntax, with a professional tone maintained throughout. Strong action verbs are used consistently at the beginning of most bullet points, contributing to clear and concise communication.',\n",
              "   'suggestions': ['Review the professional summary for potential opportunities to consolidate ideas for even greater conciseness, while retaining impact.',\n",
              "    'Ensure consistent capitalization for all technical terms and proper nouns throughout the document.']},\n",
              "  'active_writing_style': {'score': 17,\n",
              "   'explanation': \"The resume predominantly uses an active voice, starting most achievement bullets with powerful verbs such as 'Designed,' 'Built,' 'Developed,' and 'Automated.' This effectively conveys the candidate's direct involvement and contributions to the projects, making the experience sections impactful.\",\n",
              "   'suggestions': [\"Convert the remaining passive or less active phrases, such as 'Worked on' and 'Recognized with,' into active constructions to reinforce personal agency and direct impact.\",\n",
              "    \"Prioritize action verbs that precisely reflect the candidate's role and contribution in each statement, e.g., 'Led the integration of MLOps best practices...'.\"]},\n",
              "  'total_score': 75}}"
            ]
          },
          "metadata": {},
          "execution_count": 307
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "point_content_quality = \\\n",
        "Output_content_quality[\"content_quality\"][\"quantifiable_impact\"][\"score\"] + \\\n",
        "Output_content_quality[\"content_quality\"][\"skill_demonstration\"][\"score\"] + \\\n",
        "Output_content_quality[\"content_quality\"][\"specificity_and_relevance\"][\"score\"] + \\\n",
        "Output_content_quality[\"content_quality\"][\"grammar_and_language_use\"][\"score\"] + \\\n",
        "Output_content_quality[\"content_quality\"][\"active_writing_style\"][\"score\"]"
      ],
      "metadata": {
        "id": "1_DvmUqQqjbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point_content_quality"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hldljo8UrfxQ",
        "outputId": "4ab1aad2-61ef-423c-a47d-2dfaca868895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75"
            ]
          },
          "metadata": {},
          "execution_count": 309
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>"
      ],
      "metadata": {
        "id": "W-4TchmfrgoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Structure&Format"
      ],
      "metadata": {
        "id": "YVgQJDCLtaP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are an expert HR evaluator and resume reviewer.\n",
        "\n",
        "Your task is to assess the **Structure & Formatting Quality** of the following resume.\n",
        "Evaluate it across four criteria, each worth 25 points (total 100).\n",
        "\n",
        "### Evaluation Dimensions\n",
        "\n",
        "1. **Overall Structure (0–25)**\n",
        "   - Check whether the resume contains the 5 key sections:\n",
        "     1. Contact Information\n",
        "     2. Professional Summary (optional)\n",
        "     3. Education\n",
        "     4. Experience\n",
        "     5. Skills\n",
        "   - If any required section is missing → deduct points.\n",
        "   - If all sections are present and clearly separated → full points.\n",
        "\n",
        "2. **Section Organization (0–25)**\n",
        "   - Check if sections appear in a logical and professional order:\n",
        "     1. Contact Information\n",
        "     2. Professional Summary (optional)\n",
        "     3. Education\n",
        "     4. Experience\n",
        "     5. Skills\n",
        "   - Education and Experience sections can be swapped without penalty.\n",
        "   - Deduct points if sections are out of order or repeated inconsistently.\n",
        "\n",
        "3. **Date Formatting (0–25)**\n",
        "   - Verify that dates are consistently formatted throughout the resume.\n",
        "   - The correct format (for Thailand) is **DD/MM/YYYY** or **MM/YYYY** when simplified.\n",
        "   - Deduct points if date formats are inconsistent (e.g., “Jan 2021 – Feb. 22”).\n",
        "   - Deduct more if dates are missing or unclear.\n",
        "\n",
        "4. **Length & Readability (0–25)**\n",
        "   - Ideal length: approximately **2 pages**.\n",
        "   - The text should be easy to read (balanced spacing, concise bullets, not overly dense).\n",
        "   - Deduct points for overly long (3+ pages) or too short (<1 page) resumes.\n",
        "   - Deduct if sentences are too long, repetitive, or poorly formatted for readability.\n",
        "\n",
        "---\n",
        "\n",
        "### **Scoring Guide**\n",
        "- 0–10 → Poor\n",
        "- 11–17 → Fair\n",
        "- 18–22 → Good\n",
        "- 23–25 → Excellent\n",
        "\n",
        "---\n",
        "\n",
        "### **Return JSON Only**\n",
        "Return valid JSON in this format (no explanations outside the JSON):\n",
        "\n",
        "{{\n",
        "  \"structure_and_formatting\": {{\n",
        "    \"overall_structure\": {{\n",
        "      \"score\": <0-25>,\n",
        "      \"explanation\": \"\"\n",
        "    }},\n",
        "    \"section_organization\": {{\n",
        "      \"score\": <0-25>,\n",
        "      \"explanation\": \"\"\n",
        "    }},\n",
        "    \"date_formatting\": {{\n",
        "      \"score\": <0-25>,\n",
        "      \"explanation\": \"\"\n",
        "    }},\n",
        "    \"length_and_readability\": {{\n",
        "      \"score\": <0-25>,\n",
        "      \"explanation\": \"\"\n",
        "    }},\n",
        "    \"total_structure_formatting_score\": <0-100>\n",
        "  }}\n",
        "}}\n",
        "\n",
        "### **Resume Data**\n",
        "{resume_text}\n",
        "\"\"\"\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt\n",
        ")\n"
      ],
      "metadata": {
        "id": "JU2zmKflrhLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsaxJjMst-T8",
        "outputId": "7a289c6e-a942-4d56-9b52-d7e6b9ad85bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"structure_and_formatting\": {\n",
            "    \"overall_structure\": {\n",
            "      \"score\": 25,\n",
            "      \"explanation\": \"All 5 key sections (Contact Information, Professional Summary, Education, Experience, Skills) are present and clearly separated by headings.\"\n",
            "    },\n",
            "    \"section_organization\": {\n",
            "      \"score\": 25,\n",
            "      \"explanation\": \"Sections are organized in a logical and professional order: Contact, Summary, Work Experience, Projects, Professional skills, Education, Certificates & Courses, Soft Skills. The swap between Experience and Education is acceptable.\"\n",
            "    },\n",
            "    \"date_formatting\": {\n",
            "      \"score\": 21,\n",
            "      \"explanation\": \"Dates are consistently formatted as 'Month YYYY - Month YYYY' or 'Month YYYY - present'. This provides clarity. However, there is a minor typo ('preset' instead of 'present'). Also, the format does not strictly adhere to the 'DD/MM/YYYY' or 'MM/YYYY' numerical format specified for Thailand, though 'Month YYYY' is a globally accepted professional standard for date representation. Deductions for the typo and the slight deviation from the specified numerical format.\"\n",
            "    },\n",
            "    \"length_and_readability\": {\n",
            "      \"score\": 22,\n",
            "      \"explanation\": \"The resume appears to be an appropriate length, likely fitting within 1.5 to 2 pages. Readability is generally good due to the use of bullet points in the Work Experience and Projects sections, and categorized skills. However, the 'Summary' and 'Soft Skills' sections are presented as denser paragraphs, which could be made more scannable with shorter sentences or bullet points for soft skills. Some bullet points could also be slightly more concise.\"\n",
            "    },\n",
            "    \"total_structure_formatting_score\": 93\n",
            "  }\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Output_structure_n_format = json.loads(response.text.split('```json')[1].split('```')[0])\n",
        "Output_structure_n_format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN_VUeYluFfs",
        "outputId": "1512283e-e627-4eb3-896a-d2370a147087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'structure_and_formatting': {'overall_structure': {'score': 25,\n",
              "   'explanation': 'All 5 key sections (Contact Information, Professional Summary, Education, Experience, Skills) are present and clearly separated by headings.'},\n",
              "  'section_organization': {'score': 25,\n",
              "   'explanation': 'Sections are organized in a logical and professional order: Contact, Summary, Work Experience, Projects, Professional skills, Education, Certificates & Courses, Soft Skills. The swap between Experience and Education is acceptable.'},\n",
              "  'date_formatting': {'score': 21,\n",
              "   'explanation': \"Dates are consistently formatted as 'Month YYYY - Month YYYY' or 'Month YYYY - present'. This provides clarity. However, there is a minor typo ('preset' instead of 'present'). Also, the format does not strictly adhere to the 'DD/MM/YYYY' or 'MM/YYYY' numerical format specified for Thailand, though 'Month YYYY' is a globally accepted professional standard for date representation. Deductions for the typo and the slight deviation from the specified numerical format.\"},\n",
              "  'length_and_readability': {'score': 22,\n",
              "   'explanation': \"The resume appears to be an appropriate length, likely fitting within 1.5 to 2 pages. Readability is generally good due to the use of bullet points in the Work Experience and Projects sections, and categorized skills. However, the 'Summary' and 'Soft Skills' sections are presented as denser paragraphs, which could be made more scannable with shorter sentences or bullet points for soft skills. Some bullet points could also be slightly more concise.\"},\n",
              "  'total_structure_formatting_score': 93}}"
            ]
          },
          "metadata": {},
          "execution_count": 312
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "point_structure_n_format = \\\n",
        "Output_structure_n_format[\"structure_and_formatting\"][\"overall_structure\"][\"score\"] + \\\n",
        "Output_structure_n_format[\"structure_and_formatting\"][\"section_organization\"][\"score\"] + \\\n",
        "Output_structure_n_format[\"structure_and_formatting\"][\"date_formatting\"][\"score\"] + \\\n",
        "Output_structure_n_format[\"structure_and_formatting\"][\"length_and_readability\"][\"score\"]\n"
      ],
      "metadata": {
        "id": "X84nm3AZuM9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point_structure_n_format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBib_P2pulMZ",
        "outputId": "2eaae5c6-029f-4991-f031-e68d7df26fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93"
            ]
          },
          "metadata": {},
          "execution_count": 314
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>"
      ],
      "metadata": {
        "id": "HuccoKxpumcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overall"
      ],
      "metadata": {
        "id": "BcoGBzvOusyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "point_overall = \\\n",
        "point_role_relevance + \\\n",
        "point_section_level_criteria + \\\n",
        "point_content_quality + \\\n",
        "point_structure_n_format"
      ],
      "metadata": {
        "id": "xq0T_HYwunbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grade_resume(total_score: float) -> str:\n",
        "    score = max(0, min(total_score, 400))\n",
        "    pct = (score / 400) * 100\n",
        "    if pct == 100:\n",
        "        return \"S\"\n",
        "    elif pct >= 97:\n",
        "        return \"A+\"\n",
        "    elif pct >= 93:\n",
        "        return \"A\"\n",
        "    elif pct >= 90:\n",
        "        return \"A-\"\n",
        "    elif pct >= 87:\n",
        "        return \"B+\"\n",
        "    elif pct >= 83:\n",
        "        return \"B\"\n",
        "    elif pct >= 80:\n",
        "        return \"B-\"\n",
        "    elif pct >= 77:\n",
        "        return \"C+\"\n",
        "    elif pct >= 73:\n",
        "        return \"C\"\n",
        "    elif pct >= 70:\n",
        "        return \"C-\"\n",
        "    elif pct >= 67:\n",
        "        return \"D+\"\n",
        "    elif pct >= 63:\n",
        "        return \"D\"\n",
        "    elif pct >= 60:\n",
        "        return \"D-\"\n",
        "    else:\n",
        "        return \"F\""
      ],
      "metadata": {
        "id": "vx4q31IwvkjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Resume : {pdf_path} -> {grade_resume(point_overall)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Uup9e8zwtr6",
        "outputId": "9c3dc890-3fa0-4e8e-fcc9-322b53cbe976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resume : /content/05_resume.pdf -> C-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<pre>\n",
        "Resume : /content/01_resume_juanjosecarin.pdf -> D+\n",
        "Resume : /content/02_Bhavesh_Wadhwani_Resume.pdf -> D\n",
        "Resume : /content/03_resume.pdf -> C-\n",
        "Resume : /content/04_shao-yuchen.pdf -> D\n",
        "Resume : /content/05_resume.pdf -> C-\n",
        "</pre>"
      ],
      "metadata": {
        "id": "82ubspqJxU-X"
      }
    }
  ]
}